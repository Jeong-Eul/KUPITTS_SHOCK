{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa47b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1886d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from pytorch_metric_learning import losses\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "from torcheval.metrics.functional.classification import multiclass_recall\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "from torcheval.metrics.functional import multiclass_auprc\n",
    "from torcheval.metrics.functional import multiclass_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ffe285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 220604 #9861\n",
    "seed_everything(seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contrastive_Embedding(nn.Module): #supervised contrastive learning\n",
    "    def __init__(self, input_size, drop_rate, hidden_unit_sizes):\n",
    "        super(Contrastive_Embedding, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_unit_sizes = hidden_unit_sizes\n",
    "        \n",
    "        leaky_relu = nn.LeakyReLU()\n",
    "        \n",
    "        encoder = [nn.Linear(input_size, hidden_unit_sizes[0]),\n",
    "                   nn.BatchNorm1d(hidden_unit_sizes[0]),\n",
    "                   nn.Dropout(drop_rate),\n",
    "                   leaky_relu]\n",
    "    \n",
    "        \n",
    "        for i in range(1,len(hidden_unit_sizes)):\n",
    "        \n",
    "                encoder.append(nn.Linear(hidden_unit_sizes[i-1], hidden_unit_sizes[i]))\n",
    "                encoder.append(nn.BatchNorm1d(hidden_unit_sizes[i]))\n",
    "                encoder.append(nn.Dropout(drop_rate))\n",
    "                encoder.append(leaky_relu)\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            *encoder\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562287f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  pd.read_csv('./1hr Shock Research type27 for Cont_delta preprocessed GAP Mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3606695",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.stay_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Data type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[dataset['LR_NS']==0])/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index: Gap Shock\n",
    "gap_shock_index = dataset[(dataset['Binary Shock']==1) & (dataset['Shock']==0)].index\n",
    "#index: pre-Defined Shock\n",
    "pre_definded_shock_index = dataset[(dataset['Binary Shock']==1) & (dataset['Shock']>0)].index\n",
    "\n",
    "dataset['Target Shock'] = 0\n",
    "dataset['Target Shock'][gap_shock_index] = 1\n",
    "dataset['Target Shock'][pre_definded_shock_index] = 2\n",
    "\n",
    "Propofol_index = dataset[dataset['Propofol']>0].index\n",
    "Midazolam_index =dataset[dataset['Midazolam']>0].index\n",
    "Fentanyl_index =dataset[dataset['Fentanyl']>0].index\n",
    "\n",
    "dataset['Propofol'][Propofol_index] = 1\n",
    "dataset['Midazolam'][Midazolam_index] = 1\n",
    "dataset['Fentanyl'][Fentanyl_index] = 1\n",
    "\n",
    "norepi_index = dataset[dataset['Norepinephrine']>0].index\n",
    "epi_index = dataset[dataset['Epinephrine']>0].index\n",
    "phenyl_index = dataset[dataset['Phenylephrine']>0].index\n",
    "vaso_index = dataset[dataset['Vasopressin']>0].index\n",
    "\n",
    "dataset['Norepinephrine'][norepi_index] = 1\n",
    "dataset['Epinephrine'][epi_index] = 1\n",
    "dataset['Phenylephrine'][phenyl_index] = 1\n",
    "dataset['Vasopressin'][vaso_index] = 1\n",
    "\n",
    "vaso_on_index = dataset[dataset['Vasopressors']>0].index\n",
    "dataset['Vasopressors:Binary'] = 0\n",
    "dataset['Vasopressors:Binary'][vaso_on_index] = 1\n",
    "\n",
    "lr_ns_on_index = dataset[dataset['LR_NS']>0].index\n",
    "dataset['LR_NS:Binary'] = 0\n",
    "dataset['LR_NS:Binary'][lr_ns_on_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[dataset['LR_NS']>0].shape[0]/(dataset[dataset['LR_NS']>0].shape[0]+dataset[dataset['LR_NS']==0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680aae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Elevation index type'] = dataset['Elevation index type'].fillna(27).astype('int')\n",
    "\n",
    "dataset['Cont_label'] = None\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==0) & (dataset['Target Shock']==0)].index] = 0\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==0) & (dataset['Target Shock']==1)].index] = 1\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==0) & (dataset['Target Shock']==2)].index] = 2\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==1) & (dataset['Target Shock']==0)].index] = 3\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==1) & (dataset['Target Shock']==1)].index] = 4\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==1) & (dataset['Target Shock']==2)].index] = 5\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==2) & (dataset['Target Shock']==0)].index] = 6\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==2) & (dataset['Target Shock']==1)].index] = 7\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==2) & (dataset['Target Shock']==2)].index] = 8\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==3) & (dataset['Target Shock']==0)].index] = 9\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==3) & (dataset['Target Shock']==1)].index] = 10\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==3) & (dataset['Target Shock']==2)].index] = 11\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==4) & (dataset['Target Shock']==0)].index] = 12\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==4) & (dataset['Target Shock']==1)].index] = 13\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==4) & (dataset['Target Shock']==2)].index] = 14\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==5) & (dataset['Target Shock']==0)].index] = 15\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==5) & (dataset['Target Shock']==1)].index] = 16\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==5) & (dataset['Target Shock']==2)].index] = 17\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==6) & (dataset['Target Shock']==0)].index] = 18\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==6) & (dataset['Target Shock']==1)].index] = 19\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==6) & (dataset['Target Shock']==2)].index] = 20\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==7) & (dataset['Target Shock']==0)].index] = 21\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==7) & (dataset['Target Shock']==1)].index] = 22\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==7) & (dataset['Target Shock']==2)].index] = 23\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==8) & (dataset['Target Shock']==0)].index] = 24\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==8) & (dataset['Target Shock']==1)].index] = 25\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==8) & (dataset['Target Shock']==2)].index] = 26\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==9) & (dataset['Target Shock']==0)].index] = 27\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==9) & (dataset['Target Shock']==1)].index] = 28\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==9) & (dataset['Target Shock']==2)].index] = 29\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==10) & (dataset['Target Shock']==0)].index] = 30\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==10) & (dataset['Target Shock']==1)].index] = 31\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==10) & (dataset['Target Shock']==2)].index] = 32\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==11) & (dataset['Target Shock']==0)].index] = 33\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==11) & (dataset['Target Shock']==1)].index] = 34\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==11) & (dataset['Target Shock']==2)].index] = 35\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==12) & (dataset['Target Shock']==0)].index] = 36\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==12) & (dataset['Target Shock']==1)].index] = 37\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==12) & (dataset['Target Shock']==2)].index] = 38\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==13) & (dataset['Target Shock']==0)].index] = 39\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==13) & (dataset['Target Shock']==1)].index] = 40\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==13) & (dataset['Target Shock']==2)].index] = 41\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==14) & (dataset['Target Shock']==0)].index] = 42\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==14) & (dataset['Target Shock']==1)].index] = 43\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==14) & (dataset['Target Shock']==2)].index] = 44\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==15) & (dataset['Target Shock']==0)].index] = 45\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==15) & (dataset['Target Shock']==1)].index] = 46\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==15) & (dataset['Target Shock']==2)].index] = 47\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==16) & (dataset['Target Shock']==0)].index] = 48\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==16) & (dataset['Target Shock']==1)].index] = 49\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==16) & (dataset['Target Shock']==2)].index] = 50\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==17) & (dataset['Target Shock']==0)].index] = 51\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==17) & (dataset['Target Shock']==1)].index] = 52\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==17) & (dataset['Target Shock']==2)].index] = 53\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==18) & (dataset['Target Shock']==0)].index] = 54\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==18) & (dataset['Target Shock']==1)].index] = 55\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==18) & (dataset['Target Shock']==2)].index] = 56\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==19) & (dataset['Target Shock']==0)].index] = 57\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==19) & (dataset['Target Shock']==1)].index] = 58\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==19) & (dataset['Target Shock']==2)].index] = 59\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==20) & (dataset['Target Shock']==0)].index] = 60\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==20) & (dataset['Target Shock']==1)].index] = 61\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==20) & (dataset['Target Shock']==2)].index] = 62\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==21) & (dataset['Target Shock']==0)].index] = 63\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==21) & (dataset['Target Shock']==1)].index] = 64\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==21) & (dataset['Target Shock']==2)].index] = 65\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==22) & (dataset['Target Shock']==0)].index] = 66\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==22) & (dataset['Target Shock']==1)].index] = 67\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==22) & (dataset['Target Shock']==2)].index] = 68\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==23) & (dataset['Target Shock']==0)].index] = 69\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==23) & (dataset['Target Shock']==1)].index] = 70\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==23) & (dataset['Target Shock']==2)].index] = 71\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==24) & (dataset['Target Shock']==0)].index] = 72\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==24) & (dataset['Target Shock']==1)].index] = 73\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==24) & (dataset['Target Shock']==2)].index] = 74\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==25) & (dataset['Target Shock']==0)].index] = 75\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==25) & (dataset['Target Shock']==1)].index] = 76\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==25) & (dataset['Target Shock']==2)].index] = 77\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==26) & (dataset['Target Shock']==0)].index] = 78\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==26) & (dataset['Target Shock']==1)].index] = 79\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==26) & (dataset['Target Shock']==2)].index] = 80\n",
    "\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==27) & (dataset['Target Shock']==0)].index] = 81\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==27) & (dataset['Target Shock']==1)].index] = 82\n",
    "dataset['Cont_label'][dataset[(dataset['Elevation index type']==27) & (dataset['Target Shock']==2)].index] = 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc193ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef538f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param():\n",
    "    USER_NAME = 'JH'                  \n",
    "    SEED      = 220604 #9861\n",
    "    batch_size= 128\n",
    "    epoch     = 120\n",
    "    lr        = 0\n",
    "    drop_rate = 0.223573505016481\n",
    "    hidden = {\n",
    "        'hidden' : []\n",
    "        \n",
    "     }\n",
    "    temp = 0.3137977865167101\n",
    "    return hidden, SEED, batch_size, lr, epoch, drop_rate, temp\n",
    "\n",
    "\n",
    "def load_data(df):\n",
    "    '''x = df.drop(['subject_id', 'hadm_id', 'stay_id', 'time', \n",
    "                 'Death', 'gender', 'anchor_age', 'Weight', 'Height',\n",
    "                 'Elevation index type', 'Shock', 'Ambiguous Shock',\n",
    "                 'LR_NS', 'Vasopressors', 'Transfusion',\n",
    "                 'Actual Troponin test', 'Actual Creatinine test', 'Actual Lactate test', \n",
    "                 'Data type', 'Readmission','CMO', 'Cont_label','Hemoglobin', 'Creatinine', 'Troponin', 'Lactate',\n",
    "                 'IV fluids', 'Urine Output',\n",
    "                 'Troponin Elevation index', 'Creatinine Elevation index',\n",
    "                 'Lactate Elevation index','Binary Shock', 'Propofol', 'Midazolam', 'Fentanyl', 'Ketamine', 'Epinephrine',\n",
    "                 'Norepinephrine', 'Phenylephrine', 'Vasopressin', 'Target Shock'], axis= 1)'''\n",
    "    \n",
    "    x = df[['Heart rate', 'Blood pressure systolic', 'Blood pressure diastolic', 'Blood pressure mean', 'Respiratory rate', 'SpO2', 'Temperature', 'Shock index',\n",
    "            'Heart rate_RSI', 'Blood pressure systolic_RSI', 'Blood pressure diastolic_RSI', 'Blood pressure mean_RSI', 'Respiratory rate_RSI', 'SpO2_RSI', 'Temperature_RSI', 'Shock index_RSI',\n",
    "            'Heart rate_delta', 'Heart rate_delta_ratio', 'Blood pressure systolic_delta', 'Blood pressure systolic_delta_ratio', 'Blood pressure diastolic_delta', 'Blood pressure diastolic_delta_ratio', 'Blood pressure mean_delta',\n",
    "            'Blood pressure mean_delta_ratio', 'Respiratory rate_delta', 'Respiratory rate_delta_ratio', 'SpO2_delta', 'SpO2_delta_ratio', 'Temperature_delta', 'Temperature_delta_ratio', 'Shock index_delta', 'Shock index_delta_ratio',\n",
    "            'Creatinine', 'Troponin', 'Lactate', 'Actual Troponin test', 'Actual Creatinine test', 'Actual Lactate test', 'Troponin Elevation index', 'Creatinine Elevation index', 'Lactate Elevation index', 'Readmission', 'Elevation index type',\n",
    "            'Propofol', 'Midazolam', 'Fentanyl', 'Ketamine']] #, 'Vasopressors:Binary', 'LR_NS:Binary'\n",
    "\n",
    "    fn = x.shape[1]\n",
    "    print(f'{fn} Features: ', x.columns)\n",
    "    #x['Ketamine'] = x['Ketamine'].astype('category')\n",
    "    #x['Propofol'] = x['Propofol'].astype('category')\n",
    "    #x['Midazolam'] = x['Midazolam'].astype('category')\n",
    "    #x['Fentanyl'] = x['Fentanyl'].astype('category')\n",
    "    #x['Troponin Elevation index'] = x['Troponin Elevation index'].astype('category')\n",
    "    #x['Creatinine Elevation index'] = x['Creatinine Elevation index'].astype('category')\n",
    "    #x['Lactate Elevation index'] = x['Lactate Elevation index'].astype('category')\n",
    "\n",
    "    y = df['Cont_label'].astype('category') # y = df['Target Shock'].astype('category')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train(trial, search = True):\n",
    "    global emb_model, results_loss, optimizer, scheduler\n",
    "    \n",
    "    hidden, SEED, batch_size, lr, epoch, drop_rate, temp = param()\n",
    "    \n",
    "    # search parameters\n",
    "    if search == True:\n",
    "        for idx, i in enumerate(hidden):\n",
    "            num_layers = trial.suggest_int(f'num_layer_{idx}', 3, 10) # layer 수\n",
    "            '''for i in range(num_layers):\n",
    "                hidden['hidden'].append(trial.suggest_int(f'h{i+1}', 10, 200)) # node 수'''\n",
    "\n",
    "            for i in range(num_layers-1):\n",
    "                hidden['hidden'].append(trial.suggest_int(f'h{i+1}', 10, 500)) # node 수\n",
    "            hidden['hidden'].append(trial.suggest_int(f'h{num_layers}', 5, 20)) # last layer node 수\n",
    "        epoch     = 300 #trial.suggest_int('epoch', 70, 200)\n",
    "        lr        = trial.suggest_categorical('Learning_rate',[0.0001, 0.00005, 0.00001])\n",
    "        #drop_rate = trial.suggest_uniform('drop_rate', 0.56, 0.65)\n",
    "        drop_rate = 0\n",
    "        temp      = trial.suggest_uniform('temp', 0.1, 0.5)\n",
    "\n",
    "    else :\n",
    "        num_layers = trial.params['num_layer_0']\n",
    "        hidden['hidden'] = [trial.params['h1'],trial.params['h2'],trial.params['h3'],trial.params['h4'],trial.params['h5']] #,trial.params['h9'],trial.params['h10']\n",
    "        #hidden['hidden'] = [147, 42, 91, 145, 92, 69, 165, 78, 39]\n",
    "        epoch = 300 #trial.params['epoch']\n",
    "        lr = 0.00001 #trial.params['Learning_rate']\n",
    "        #drop_rate = trial.params['drop_rate']\n",
    "        drop_rate = 0\n",
    "        temp = trial.params['temp']\n",
    "\n",
    "    # hidden['hidden'] = sorted(hidden['hidden'], reverse=True)       \n",
    "    print(hidden)\n",
    "    print('learning_rate : ', lr, \"\\nepoch : \", epoch, \"\\ndrop_rate : \", drop_rate, \"\\ntemperature : \", temp)\n",
    "    \n",
    "    results_loss = {\n",
    "    'epoch_by_trn'          : [],\n",
    "    'epoch_by_val'          : []\n",
    "    }\n",
    "    mimic_df = dataset.copy()\n",
    "    \n",
    "    #undersampling\n",
    "    #circ가 한번이라도 오지 않은 환자 언더 샘플링\n",
    "    \n",
    "    # sty=[]\n",
    "    # for stay_id, group_data in mimic_df.groupby('stay_id'):\n",
    "    #     if (group_data['classes'] == 0).all():\n",
    "    #         sty.append(stay_id)\n",
    "    \n",
    "    # mimic_df = mimic_df[~(mimic_df.stay_id.isin(sty))]\n",
    "    \n",
    "    # print(mimic_df['classes'].value_counts().sort_index())\n",
    "    \n",
    "    trn_x, trn_y  = load_data(mimic_df)\n",
    "    \n",
    "    # data leakage 조심\n",
    "    scaler        = MinMaxScaler()\n",
    "    trn_sclaed_x  = scaler.fit_transform(trn_x)\n",
    "\n",
    "    trn_sclaed_x[np.isnan(trn_sclaed_x)] = -1000\n",
    "\n",
    "    trn_tensor_x  = torch.FloatTensor(trn_sclaed_x)\n",
    "    trn_tensor_y  = torch.LongTensor(trn_y.values) \n",
    "    \n",
    "  \n",
    "    \n",
    "    n_feat = trn_tensor_x.shape[1]\n",
    "        \n",
    "    train_dataset = TensorDataset(trn_tensor_x, trn_tensor_y)\n",
    "\n",
    "\n",
    "    \n",
    "    y_train_indices = mimic_df.index\n",
    "\n",
    "\n",
    "    y_train = [mimic_df['Cont_label'][i] for i in y_train_indices]\n",
    "\n",
    "\n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "    \n",
    "\n",
    "\n",
    "    weight = 1. / class_sample_count\n",
    "    \n",
    "    samples_weight = np.array([weight[t] for t in y_train])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    \n",
    "\n",
    "\n",
    "    sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "    train_loader  = torch.utils.data.DataLoader(dataset= train_dataset, batch_size=batch_size, shuffle=False, sampler=sampler, drop_last=True)\n",
    "\n",
    "    # 모델 정의\n",
    "    emb_model = Contrastive_Embedding(n_feat, drop_rate, hidden['hidden']).to(device)\n",
    "    print(emb_model)\n",
    "    \n",
    "    contrastive_loss = losses.SupConLoss(temperature=temp)\n",
    "    # mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.RMSprop(emb_model.parameters(), lr= 0)\n",
    "    \n",
    "    scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=50, T_mult=1, eta_max=0.0001,  T_up=50, gamma=0.5)\n",
    "    \n",
    "    patience_value = 0\n",
    "    patience = 10\n",
    "    for i in range(1, epoch+1):\n",
    "        emb_model.train()\n",
    "        \n",
    "        train_loss = {'cont_loss' :[],\n",
    "                      'mse_loss'  :[],\n",
    "                      'total_loss':[],\n",
    "                      'epoch_by'  :[]}\n",
    "        \n",
    "        # current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        start = time.time()\n",
    "        \n",
    "        for j, (X, y)  in enumerate(train_loader):\n",
    "            \n",
    "            X  = X.to(device)\n",
    "            y  = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            latent = emb_model.forward(X)\n",
    "            \n",
    "            # loss_mse = mse_loss(out, X)\n",
    "            \n",
    "            y =  y.type(torch.LongTensor)\n",
    "            loss_cont = contrastive_loss(latent, y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss_cont.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss['cont_loss'].append(loss_cont.item())\n",
    "\n",
    "        tr_cont_mean = np.array(train_loss['cont_loss']).mean()\n",
    "        \n",
    "        scheduler.step()  \n",
    "        results_loss['epoch_by_trn'].append(tr_cont_mean)\n",
    "        \n",
    "        end = time.time()\n",
    "        if i % 1 == 0:\n",
    "            print(f'epoch {i}  time: {end - start:.4f}sec trn_contrastive: {tr_cont_mean: .4f}')\n",
    "\n",
    "        if i > 2:\n",
    "            if results_loss['epoch_by_trn'][-1] > results_loss['epoch_by_trn'][-2]:\n",
    "                patience_value += 1\n",
    "\n",
    "        if patience_value == patience :\n",
    "            print('------------------------------------------------')\n",
    "            print(f'epoch {i} End')\n",
    "            print('================================================')\n",
    "            patience_value = 0\n",
    "            break\n",
    "    \n",
    "    return tr_cont_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeded_df(model_name): \n",
    "    print()\n",
    "    print('Start Getting the latent space vector(Train, Valid sample)')\n",
    "    \n",
    "    mimic_df = dataset.copy()\n",
    "    print(len(mimic_df))\n",
    "    \n",
    "    trn_x, trn_y  = load_data(mimic_df)\n",
    "    \n",
    "    scaler        = MinMaxScaler()\n",
    "    trn_sclaed_x  = scaler.fit_transform(trn_x)\n",
    "\n",
    "    trn_sclaed_x[np.isnan(trn_sclaed_x)] = -10000\n",
    "\n",
    "    trn_tensor_x  = torch.FloatTensor(trn_sclaed_x)\n",
    "    trn_tensor_y  = torch.LongTensor(trn_y.values) \n",
    "    \n",
    "    \n",
    "    n_feat = trn_tensor_x.shape[1]\n",
    "        \n",
    "    train_dataset = TensorDataset(trn_tensor_x, trn_tensor_y)\n",
    "    for_latent_loader_trn  = torch.utils.data.DataLoader(dataset= train_dataset, batch_size=trn_tensor_x.shape[0], shuffle=False, drop_last=False)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    model_name.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_l, y_l  in for_latent_loader_trn: # Full batch\n",
    "                \n",
    "                X_l  = X_l.to(device)\n",
    "                latent_vector_train = model_name.forward(X_l)\n",
    "                \n",
    "                emb_train_x = pd.DataFrame(np.array(latent_vector_train.cpu()))\n",
    "                emb_train = pd.concat([emb_train_x, pd.DataFrame(np.array(y_l))], axis = 1)\n",
    "                # emb_train = pd.concat([information, emb_train], axis = 1)\n",
    "    end = time.time()            \n",
    "    print()\n",
    "    print('End, Time consume(min):{}'.format((end - start)/60))  \n",
    "    \n",
    "    return emb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba9128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']= '1'\n",
    "n_gpu             = 1\n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction=\"minimize\")\n",
    "study.optimize(train, n_trials = 5)  \n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ccb1f",
   "metadata": {},
   "source": [
    "Parameter 정보 꼭 기록하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67886bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']= '1'\n",
    "n_gpu             = 1\n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "model_loss = train(trial=trial,search = False)\n",
    "\n",
    "version = '240721'\n",
    "\n",
    "'''\n",
    "240526 is {'hidden': [136, 19, 55, 13, 153, 105, 167, 87, 34]}\n",
    "learning_rate :  0.0006 \n",
    "epoch :  86 \n",
    "drop_rate :  0 \n",
    "temperature :  0.47626381444779575\n",
    "\n",
    "240606 is \n",
    "  Value:  4.829311113724287\n",
    "  Params: \n",
    "    num_layer_0: 10\n",
    "    h1: 104\n",
    "    h2: 108\n",
    "    h3: 52\n",
    "    h4: 181\n",
    "    h5: 34\n",
    "    h6: 197\n",
    "    h7: 119\n",
    "    h8: 70\n",
    "    h9: 200\n",
    "    h10: 14\n",
    "    epoch: 89\n",
    "    Learning_rate: 0.00005\n",
    "    temp: 0.3725457792592851\n",
    "\n",
    "240616 is\n",
    " Value:  4.833700034429641\n",
    "  Params: \n",
    "    num_layer_0: 10\n",
    "    h1: 55\n",
    "    h2: 172\n",
    "    h3: 131\n",
    "    h4: 81\n",
    "    h5: 104\n",
    "    h6: 138\n",
    "    h7: 18\n",
    "    h8: 27\n",
    "    h9: 32\n",
    "    h10: 7\n",
    "    Learning_rate: 0.0001\n",
    "    temp: 0.19152583769744558    \n",
    "\n",
    "240622 is\n",
    "Value:  4.813156143596138\n",
    "  Params: \n",
    "    num_layer_0: 10\n",
    "    h1: 22\n",
    "    h2: 197\n",
    "    h3: 174\n",
    "    h4: 116\n",
    "    h5: 116\n",
    "    h6: 60\n",
    "    h7: 70\n",
    "    h8: 183\n",
    "    h9: 123\n",
    "    h10: 14\n",
    "    Learning_rate: 0.0001\n",
    "    temp: 0.4484512259552573\n",
    "\n",
    "240624 is\n",
    "Value:  4.234910612705803\n",
    "  Params: \n",
    "    num_layer_0: 10\n",
    "    h1: 283\n",
    "    h2: 191\n",
    "    h3: 145\n",
    "    h4: 113\n",
    "    h5: 41\n",
    "    h6: 497\n",
    "    h7: 17\n",
    "    h8: 427\n",
    "    h9: 84\n",
    "    h10: 6\n",
    "    Learning_rate: 0.0001\n",
    "    temp: 0.18610683430418767    \n",
    "\n",
    "240712 is\n",
    "Best trial:\n",
    "  Value:  1.7689395768463896\n",
    "  Params: \n",
    "    num_layer_0: 8\n",
    "    h1: 53\n",
    "    h2: 385\n",
    "    h3: 300\n",
    "    h4: 490\n",
    "    h5: 304\n",
    "    h6: 458\n",
    "    h7: 209\n",
    "    h8: 20\n",
    "    Learning_rate: 1e-05\n",
    "    temp: 0.2572784082698316\n",
    "\n",
    "  240721 is\n",
    "  Best trial:\n",
    "  Value:  1.6679313899698696\n",
    "  Params: \n",
    "    num_layer_0: 5\n",
    "    h1: 66\n",
    "    h2: 30\n",
    "    h3: 268\n",
    "    h4: 15\n",
    "    h5: 19\n",
    "    Learning_rate: 0.0001\n",
    "    temp: 0.1768852090437697s\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "for i in range(120):\n",
    "    optimizer.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "plt.plot(range(120), lrs, color = 'limegreen',  label = 'Training Cosine Annealing Warm Restarts')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate(green)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(69), results_loss['epoch_by_trn'], color = 'dodgerblue', label = 'Contrastive Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./vital_lab_{version}_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": emb_model,\n",
    "            },\n",
    "           f\"./Contrastive_Embedding_Net_vital_lab_({version}).pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = make_embeded_df(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e742e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.concat([dataset.reset_index(drop=True), emb_df.reset_index(drop=True)], axis = 1, ignore_index=False)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.stay_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41390a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(f'./train vital embedding_data_type27_{version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(f'./train vital embedding_data_type27_{version}.csv')\n",
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['Cont_label'] = sample['0.1']\n",
    "sample.drop(columns=['0.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e561aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(f'./train vital embedding_data_type27_{version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186dfda",
   "metadata": {},
   "source": [
    "T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "from matplotlib import colors\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "x = sample.drop(columns=['subject_id', 'hadm_id', 'stay_id', 'time', 'Death', 'gender',\n",
    "       'anchor_age', 'Weight', 'Height', 'Heart rate',\n",
    "       'Blood pressure systolic', 'Blood pressure diastolic',\n",
    "       'Blood pressure mean', 'Respiratory rate', 'SpO2', 'Temperature',\n",
    "       'Hemoglobin', 'Creatinine', 'Troponin', 'Lactate', \n",
    "       'Propofol', 'Midazolam', 'Fentanyl',\n",
    "       'IV fluids', 'Transfusion', 'Urine Output', 'LR_NS', 'Shock', 'Ambiguous Shock',\n",
    "       'Ketamine', 'Vasopressors', 'Actual Troponin test',\n",
    "       'Actual Creatinine test', 'Actual Lactate test',\n",
    "       'Troponin Elevation index', 'Creatinine Elevation index',\n",
    "       'Lactate Elevation index', 'Readmission', 'Elevation index type',\n",
    "       'Heart rate_delta', 'Heart rate_delta_ratio',\n",
    "       'Blood pressure systolic_delta', 'Blood pressure systolic_delta_ratio',\n",
    "       'Blood pressure diastolic_delta',\n",
    "       'Blood pressure diastolic_delta_ratio', 'Blood pressure mean_delta',\n",
    "       'Blood pressure mean_delta_ratio', 'Respiratory rate_delta',\n",
    "       'Respiratory rate_delta_ratio', 'SpO2_delta', 'SpO2_delta_ratio',\n",
    "       'Temperature_delta', 'Temperature_delta_ratio', 'Shock index_delta', 'Shock index_delta_ratio',\n",
    "       'Data type', 'CMO', 'Epinephrine', 'Norepinephrine', 'Phenylephrine', 'Vasopressin', 'Cont_label','Binary Shock','Shock index', 'Target Shock',\n",
    "       'Heart rate_RSI', 'Blood pressure systolic_RSI',\n",
    "       'Blood pressure diastolic_RSI', 'Blood pressure mean_RSI',\n",
    "       'Respiratory rate_RSI', 'SpO2_RSI', 'Temperature_RSI', 'Shock index_RSI',\n",
    "       'Vasopressors:Binary', 'LR_NS:Binary','peri_Shock'])\n",
    "\n",
    "\n",
    "y = sample[['Cont_label', 'Troponin Elevation index', 'Creatinine Elevation index', 'Lactate Elevation index', 'CMO', 'Shock', 'Binary Shock', 'Target Shock','peri_Shock']]\n",
    "\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-sne operation\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "tsne = TSNE(n_components=2,random_state = 0)\n",
    "\n",
    "result_tsne = tsne.fit_transform(x)\n",
    "\n",
    "# 시각화\n",
    "#all_colors = list(colors.CSS4_COLORS.keys())\n",
    "#selected_colors = random.sample(all_colors, state_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Cont_label'], s=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "#plt.title(\"T-sne Visualization of {} States\".format(state_num), fontsize=16)\n",
    "#plt.savefig(f\"C:/Users/DAHS/Desktop/KU X PITTS/Cont data/1 T-sne Visualization of vital_{version} Target Shock\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Target Shock'], s=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "#plt.title(\"T-sne Visualization of {} States\".format(state_num), fontsize=16)\n",
    "#plt.savefig(f\"C:/Users/DAHS/Desktop/KU X PITTS/Cont data/1 T-sne Visualization of vital_{version} Target Shock\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_index = sample[(sample['Data type']=='Non Ambiguous')].index\n",
    "ambiguous_index = sample[(sample['Data type']=='Ambiguous')].index\n",
    "\n",
    "y['Data type int'] = 0\n",
    "y['Data type int'][non_ambiguous_index] = 100\n",
    "y['Data type int'][ambiguous_index] = -100\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Data type int'], s=0.5)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"T-sne Visualization of unknown\", fontsize=16)\n",
    "#plt.savefig(f\"C:/Users/DAHS/Desktop/KU X PITTS/Cont data/2 T-sne Visualization of non_ambiguous{state_num}_{date} Elevation type Unknown\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_normal_index = sample[(sample['Data type']=='Ambiguous') & (sample['Target Shock']==0)].index\n",
    "ambiguous_peri_index = sample[(sample['Data type']=='Ambiguous') & (sample['Target Shock']==1)].index\n",
    "ambiguous_sh_index = sample[(sample['Data type']=='Ambiguous') & (sample['Target Shock']==2)].index\n",
    "\n",
    "non_ambiguous_normal_index = sample[(sample['Data type']=='Non Ambiguous') & (sample['Target Shock']==0)].index\n",
    "non_ambiguous_peri_index = sample[(sample['Data type']=='Non Ambiguous') & (sample['Target Shock']==1)].index\n",
    "non_ambiguous_sh_index = sample[(sample['Data type']=='Non Ambiguous') & (sample['Target Shock']==2)].index\n",
    "\n",
    "y['Data type int'] = 0\n",
    "\n",
    "y['Data type int'][ambiguous_normal_index] = -300\n",
    "y['Data type int'][ambiguous_peri_index] = -200\n",
    "y['Data type int'][ambiguous_sh_index] = -100\n",
    "y['Data type int'][non_ambiguous_normal_index] = 100\n",
    "y['Data type int'][non_ambiguous_peri_index] = 200\n",
    "y['Data type int'][non_ambiguous_sh_index] = 300\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Data type int'], s=0.5)\n",
    "#plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"T-sne Visualization of Ambi and non-ambi\", fontsize=16)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.savefig(f\"./T-sne Visualization of vital_lab_{version} Normal and peri and Shock\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Binary Shock'], s=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "#plt.title(\"T-sne Visualization of {} States\".format(state_num), fontsize=16)\n",
    "#plt.savefig(f\"C:/Users/DAHS/Desktop/KU X PITTS/Cont data/1 T-sne Visualization of vital_{version} States \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46304578",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Shock'], s=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "#plt.title(\"T-sne Visualization of {} States\".format(state_num), fontsize=16)\n",
    "#plt.savefig(f\"C:/Users/DAHS/Desktop/KU X PITTS/Cont data/1 T-sne Visualization of vital_{version} States \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34342d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[sample['Elevation index type']==4][['Lactate Elevation index','Creatinine Elevation index','Troponin Elevation index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_index = sample[(sample['Data type']=='Non Ambiguous') & (sample['Target Shock']==1) & (sample['Elevation index type']==4)].index\n",
    "ambiguous_index = sample[(sample['Data type']=='Ambiguous') & (sample['Target Shock']==1) & (sample['Elevation index type']==27)].index\n",
    "\n",
    "y['Data type int'] = 0\n",
    "y['Data type int'][non_ambiguous_index] = 100\n",
    "y['Data type int'][ambiguous_index] = 0\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(result_tsne[:, 0], result_tsne[:, 1], c=y['Data type int'], s=0.5)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"T-sne Visualization of unknown\", fontsize=16)\n",
    "#plt.savefig(f\"C:/Users/DAHS/Desktop/KU X PITTS/Cont data/2 T-sne Visualization of non_ambiguous{state_num}_{date} Elevation type Unknown\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KUxPITTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
