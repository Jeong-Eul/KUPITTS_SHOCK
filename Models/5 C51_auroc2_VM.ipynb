{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "#import torchsummary\n",
    "import os\n",
    "from torch.nn import SmoothL1Loss, MSELoss\n",
    "from torch.utils.data import TensorDataset, Sampler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_num = 16\n",
    "version = '240721'\n",
    "date = version\n",
    "train_data = pd.read_csv(f'./train_shock_dataset_state_cont_{version}_Action{action_num}_Reward_Mean_VM.csv')\n",
    "val_data = pd.read_csv(f'./val_shock_dataset_state_cont_{version}_Action{action_num}_Reward_Mean_VM.csv')\n",
    "# param 설정\n",
    "state_dim = 19 \n",
    "action_dim = action_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
    "       '13', '14', '15', '16', '17', '18']\n",
    "for column_name in columns_list:\n",
    "    train_data = train_data.rename(columns={f'{column_name}': f's:{column_name}'})\n",
    "\n",
    "for column_name in columns_list:\n",
    "    val_data = val_data.rename(columns={f'{column_name}': f's:{column_name}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'Action':'a:Action'}, inplace=True)\n",
    "train_data.rename(columns={'Reward':'r:Reward'}, inplace=True)\n",
    "train_data.Shock.fillna(0, inplace=True)\n",
    "shock_index = train_data[train_data['Target Shock']!=0].index\n",
    "train_data['sh:Shock'] = 0\n",
    "train_data['sh:Shock'][shock_index] = 1\n",
    "\n",
    "def define_terminal(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    x['t:Terminal'] = 0\n",
    "\n",
    "    x['t:Terminal'].loc[len(x)-1] = 1\n",
    "\n",
    "    return x\n",
    "train_data = train_data.groupby('stay_id').apply(define_terminal)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#data.rename(columns={'State':'s:State'}, inplace=True)\n",
    "val_data.rename(columns={'Action':'a:Action'}, inplace=True)\n",
    "val_data.rename(columns={'Reward':'r:Reward'}, inplace=True)\n",
    "val_data.Shock.fillna(0, inplace=True)\n",
    "shock_index = val_data[val_data['Target Shock']!=0].index\n",
    "val_data['sh:Shock'] = 0\n",
    "val_data['sh:Shock'][shock_index] = 1\n",
    "\n",
    "val_data = val_data.groupby('stay_id').apply(define_terminal)\n",
    "val_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#악화 중\n",
    "print('Shock 상태 악화 중: ', train_data[(train_data['Blood pressure mean_RSI']<=30) & (train_data['Target Shock']==2)]['Blood pressure mean'].mean())\n",
    "#약한 회복 중\n",
    "print('Shock 상태 약한 회복 중: ', train_data[(train_data['Blood pressure mean_RSI']<70) & (train_data['Blood pressure mean_RSI']>30) & (train_data['Target Shock']==2)]['Blood pressure mean'].mean())\n",
    "#강한 회복 중\n",
    "print('Shock 상태 강한 회복 중: ', train_data[(train_data['Blood pressure mean_RSI']>=70) & (train_data['Target Shock']==2)]['Blood pressure mean'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#악화 중\n",
    "print('Peri Shock 상태 악화 중: ', train_data[(train_data['Blood pressure mean_RSI']<=30) & (train_data['Target Shock']==1)]['Blood pressure mean'].mean())\n",
    "#약한 회복 중\n",
    "print('Peri Shock 상태 약한 회복 중: ', train_data[(train_data['Blood pressure mean_RSI']<70) & (train_data['Blood pressure mean_RSI']>30) & (train_data['Target Shock']==1)]['Blood pressure mean'].mean())\n",
    "#강한 회복 중\n",
    "print('Peri Shock 상태 강한 회복 중: ', train_data[(train_data['Blood pressure mean_RSI']>=70) & (train_data['Target Shock']==1)]['Blood pressure mean'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#악화 중\n",
    "print('정상 상태 악화 중: ', train_data[(train_data['Blood pressure mean_RSI']<=30) & (train_data['Target Shock']==0)]['Blood pressure mean'].mean())\n",
    "#약한 회복 중\n",
    "print('정상 상태 약한 회복 중: ', train_data[(train_data['Blood pressure mean_RSI']<70) & (train_data['Blood pressure mean_RSI']>30) & (train_data['Target Shock']==0)]['Blood pressure mean'].mean())\n",
    "#강한 회복 중\n",
    "print('정상 상태 강한 회복 중: ', train_data[(train_data['Blood pressure mean_RSI']>=70) & (train_data['Target Shock']==0)]['Blood pressure mean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Dead/Alive in terminal mode\n",
    "#control reward train\n",
    "train_data['r:Reward'] = 0\n",
    "train_data['r:Reward'][train_data[(train_data['t:Terminal']==1)].index] = 100\n",
    "train_data['r:Reward'][train_data[(train_data['Death']==1) & (train_data['t:Terminal']==1)].index] = -100\n",
    "print('Train: ', train_data['r:Reward'].unique())\n",
    "\n",
    "#control reward val\n",
    "val_data['r:Reward'] = 0\n",
    "val_data['r:Reward'][val_data[(val_data['t:Terminal']==1)].index] = 100\n",
    "val_data['r:Reward'][val_data[(val_data['Death']==1) & (val_data['t:Terminal']==1)].index] = -100\n",
    "print('Validation: ',val_data['r:Reward'].unique())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reward:Labtest\n",
    "train_data['Reward:vl'] = 0\n",
    "#Lactate\n",
    "train_data['Reward:Lactate'] = 0\n",
    "train_data['Reward:Lactate'][train_data[(train_data['Lactate']>2)&(train_data['Lactate Elevation index']==2)].index] = -10\n",
    "train_data['Reward:Lactate'][train_data[(train_data['Lactate']<=2)&(train_data['Lactate Elevation index']==-2)].index] = 10\n",
    "#Creatinine\n",
    "train_data['Reward:Creatinine'] = 0\n",
    "train_data['Reward:Creatinine'][train_data[(train_data['Creatinine']>3)&(train_data['Creatinine Elevation index']==2)].index] = -10\n",
    "train_data['Reward:Creatinine'][train_data[(train_data['Creatinine']<=3)&(train_data['Creatinine Elevation index']==-2)].index] = 10\n",
    "#Troponin\n",
    "train_data['Reward:Troponin'] = 0\n",
    "train_data['Reward:Troponin'][train_data[(train_data['Troponin']>14)&(train_data['Troponin Elevation index']==2)].index] = -10\n",
    "train_data['Reward:Troponin'][train_data[(train_data['Troponin']<=14)&(train_data['Troponin Elevation index']==-2)].index] = 10\n",
    "\n",
    "train_data['Reward:vl'] = train_data['Reward:Lactate'] + train_data['Reward:Creatinine'] + train_data['Reward:Troponin']\n",
    "\n",
    "#MAP mode\n",
    "train_data['Reward:MAP'] = 0\n",
    "#악화 중(shock)\n",
    "train_data['Reward:MAP'][train_data[(train_data['Blood pressure mean_RSI']<30) & (train_data['Target Shock']==2)].index] = -10\n",
    "#약한 회복 중(shock)\n",
    "#train_data['Reward:MAP'][train_data[(train_data['Blood pressure mean_RSI']<70) & (train_data['Blood pressure mean_RSI']>30) & (train_data['Target Shock']==2)].index] = -50\n",
    "#강한 회복 중(shock)\n",
    "train_data['Reward:MAP'][train_data[(train_data['Blood pressure mean_RSI']>70) & (train_data['Target Shock']==2)].index] = 10\n",
    "\n",
    "#악화 중(peri-shock)\n",
    "train_data['Reward:MAP'][train_data[(train_data['Blood pressure mean_RSI']<30) & (train_data['Target Shock']==1)].index] =-10\n",
    "#약한 회복 중(peri-shock)\n",
    "#train_data['Reward:MAP'][train_data[(train_data['Blood pressure mean_RSI']<70) & (train_data['Blood pressure mean_RSI']>30) & (train_data['Target Shock']==1)].index] = -50\n",
    "#강한 회복 중(peri-shock)\n",
    "train_data['Reward:MAP'][train_data[(train_data['Blood pressure mean_RSI']>70) & (train_data['Target Shock']==1)].index] = 10\n",
    "\n",
    "#Dead/Alive in terminal mode\n",
    "#control reward train\n",
    "train_data['Reward:Terminal'] = 0\n",
    "train_data['Reward:Terminal'][train_data[(train_data['t:Terminal']==1)].index] = 100\n",
    "train_data['Reward:Terminal'][train_data[(train_data['Death']==1) & (train_data['t:Terminal']==1)].index] = -100\n",
    "print('Train: ', train_data['Reward:Terminal'].unique())\n",
    "\n",
    "train_data['r:Reward'] = 0\n",
    "train_data['r:Reward'] = train_data['Reward:MAP'] + train_data['Reward:vl'] + train_data['Reward:Terminal']\n",
    "print('Train: ', train_data['r:Reward'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reward:Labtest\n",
    "val_data['Reward:vl'] = 0\n",
    "#Lactate\n",
    "val_data['Reward:Lactate'] = 0\n",
    "val_data['Reward:Lactate'][val_data[(val_data['Lactate']>2)&(val_data['Lactate Elevation index']==2)].index] = -10\n",
    "val_data['Reward:Lactate'][val_data[(val_data['Lactate']<=2)&(val_data['Lactate Elevation index']==-2)].index] = 10\n",
    "#Creatinine\n",
    "val_data['Reward:Creatinine'] = 0\n",
    "val_data['Reward:Creatinine'][val_data[(val_data['Creatinine']>3)&(val_data['Creatinine Elevation index']==2)].index] = -10\n",
    "val_data['Reward:Creatinine'][val_data[(val_data['Creatinine']<=3)&(val_data['Creatinine Elevation index']==-2)].index] = 10\n",
    "#Troponin\n",
    "val_data['Reward:Troponin'] = 0\n",
    "val_data['Reward:Troponin'][val_data[(val_data['Troponin']>14)&(val_data['Troponin Elevation index']==2)].index] = -10\n",
    "val_data['Reward:Troponin'][val_data[(val_data['Troponin']<=14)&(val_data['Troponin Elevation index']==-2)].index] = 10\n",
    "\n",
    "val_data['Reward:vl'] = val_data['Reward:Lactate'] + val_data['Reward:Creatinine'] + val_data['Reward:Troponin']\n",
    "\n",
    "#MAP mode\n",
    "val_data['Reward:MAP'] = 0\n",
    "#악화 중(shock)\n",
    "val_data['Reward:MAP'][val_data[(val_data['Blood pressure mean_RSI']<30) & (val_data['Target Shock']==2)].index] = -10\n",
    "#약한 회복 중(shock)\n",
    "#val_data['Reward:MAP'][val_data[(val_data['Blood pressure mean_RSI']<70) & (val_data['Blood pressure mean_RSI']>30) & (val_data['Target Shock']==2)].index] = -50\n",
    "#강한 회복 중(shock)\n",
    "val_data['Reward:MAP'][val_data[(val_data['Blood pressure mean_RSI']>70) & (val_data['Target Shock']==2)].index] = 10\n",
    "\n",
    "#악화 중(peri-shock)\n",
    "val_data['Reward:MAP'][val_data[(val_data['Blood pressure mean_RSI']<30) & (val_data['Target Shock']==1)].index] =-10\n",
    "#약한 회복 중(peri-shock)\n",
    "#val_data['Reward:MAP'][val_data[(val_data['Blood pressure mean_RSI']<70) & (val_data['Blood pressure mean_RSI']>30) & (val_data['Target Shock']==1)].index] = -50\n",
    "#강한 회복 중(peri-shock)\n",
    "val_data['Reward:MAP'][val_data[(val_data['Blood pressure mean_RSI']>70) & (val_data['Target Shock']==1)].index] = 10\n",
    "\n",
    "#control reward val\n",
    "val_data['Reward:Terminal'] = 0\n",
    "val_data['Reward:Terminal'][val_data[(val_data['t:Terminal']==1)].index] = 100\n",
    "val_data['Reward:Terminal'][val_data[(val_data['Death']==1) & (val_data['t:Terminal']==1)].index] = -100\n",
    "print('Validation: ',val_data['Reward:Terminal'].unique())\n",
    "\n",
    "val_data['r:Reward'] = 0\n",
    "val_data['r:Reward'] = val_data['Reward:MAP'] + val_data['Reward:vl'] + val_data['Reward:Terminal']\n",
    "print('Validation: ', val_data['r:Reward'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train reward ratio: ', train_data['r:Reward'].value_counts()/len(train_data))\n",
    "print('Validation reward ratio: ', val_data['r:Reward'].value_counts()/len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train reward min: ', train_data['r:Reward'].min())\n",
    "print('Train reward max: ', train_data['r:Reward'].max())\n",
    "print('Train reward mean: ', train_data['r:Reward'].mean())\n",
    "\n",
    "print('Validation reward min: ', val_data['r:Reward'].min())\n",
    "print('Validation reward max: ', val_data['r:Reward'].max())\n",
    "print('Validation reward mean: ', val_data['r:Reward'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(df):\n",
    "    # state, action, reward, next_state로 데이터를 나누기\n",
    "    states = df['s:State'].values.reshape(-1, 1)  # state는 1차원으로 reshape\n",
    "    actions = df['a:Action'].values\n",
    "    rewards = df['r:Reward'].values\n",
    "    next_states = df['s_prime:Next_state'].values.reshape(-1, 1)  # next_state는 1차원으로 reshape\n",
    "    terminal = df['t:Terminal'].values\n",
    "    \n",
    "    return states, actions, rewards, next_states, terminal\n",
    "\n",
    "def one_hot_encode(data, num_classes):\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on the given data.\n",
    "    :param data: Data to be encoded.\n",
    "    :param num_classes: Number of classes for one-hot encoding.\n",
    "    :return: Encoded data.\n",
    "    \"\"\"\n",
    "    encoded_data = np.zeros((len(data), num_classes))\n",
    "    for i, val in enumerate(data):\n",
    "        val = int(val)  # Ensure val is an integer\n",
    "        encoded_data[i, val] = 1\n",
    "    return encoded_data\n",
    "\n",
    "def one_hot_encode_for_val(data, num_classes):\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on the given data.\n",
    "    :param data: Data to be encoded.\n",
    "    :param num_classes: Number of classes for one-hot encoding.\n",
    "    :return: Encoded data.\n",
    "    \"\"\"\n",
    "    encoded_data = np.zeros((len(data), num_classes))\n",
    "    for i, val in enumerate(data):\n",
    "        #val = int(val)  # Ensure val is an integer\n",
    "        encoded_data[i, val] = 1\n",
    "    return encoded_data\n",
    "\n",
    "def make_transition(data):\n",
    "    df = data\n",
    "    s_col = [x for x in df if x[:2] == 's:']\n",
    "    a_col = [x for x in df if x[:2] == 'a:']\n",
    "    r_col = [x for x in df if x[:2] == 'r:']\n",
    "    t_col = [x for x in df if x[:2] == 't:']\n",
    "    \n",
    "    dict = {}\n",
    "    dict['traj'] = {}\n",
    "    data_len = 0\n",
    "    s, a, r, s2, t  = [], [], [], [], [] #\n",
    "\n",
    "    for traj in tqdm(df.stay_id.unique()):\n",
    "        df_traj = df[df['stay_id'] == traj]\n",
    "        dict['traj'][traj] = {'s': [], 'a': [], 'r': [], 't': []}\n",
    "        dict['traj'][traj]['s'] = df_traj[s_col].values.tolist()\n",
    "        dict['traj'][traj]['a'] = df_traj[a_col].values.tolist()\n",
    "        dict['traj'][traj]['r'] = df_traj[r_col[0]].values.tolist()\n",
    "        dict['traj'][traj]['t'] = df_traj[t_col].values.tolist()\n",
    "        #print(dict['traj'][traj]['s'][step:step+rolling_size])\n",
    "        step_len = len(df_traj) - 1\n",
    "        for step in range(step_len):\n",
    "            s.append(dict['traj'][traj]['s'][step])\n",
    "            a.append(dict['traj'][traj]['a'][step])\n",
    "            r.append(dict['traj'][traj]['r'][step+1])\n",
    "            s2.append(dict['traj'][traj]['s'][step+1])\n",
    "            t.append(dict['traj'][traj]['t'][step+1])\n",
    "            data_len += 1\n",
    "\n",
    "    s = torch.FloatTensor(np.float32(s))\n",
    "    a = torch.LongTensor(np.int64(a))\n",
    "    r = torch.FloatTensor(np.float32(r))\n",
    "    s2 = torch.FloatTensor(np.float32(s2))\n",
    "    t = torch.FloatTensor(np.float32(t))\n",
    "\n",
    "    return s,a,r,s2,t\n",
    "\n",
    "class CustomSampler(Sampler):\n",
    "    def __init__(self, data, batch_size, ns, target):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples_1 = ns\n",
    "        self.num_samples_0 = batch_size - ns\n",
    "        self.indices = [i for i in range(len(data)) if data[i][2].item() in target]\n",
    "        self.indices_neg = [i for i in range(len(data)) if data[i][2].item() not in target]\n",
    "        self.used_indices_neg = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        np.random.shuffle(self.indices_neg)\n",
    "\n",
    "        batch = []\n",
    "        for idx in self.indices:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.num_samples_0:\n",
    "                batch.extend(self._sample_indices_neg())\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    def _sample_indices_neg(self, remaining=0):\n",
    "        if remaining:\n",
    "            num_samples_1 = self.batch_size - remaining\n",
    "        else:\n",
    "            num_samples_1 = self.num_samples_1\n",
    "\n",
    "        if len(self.used_indices_neg) + num_samples_1 > len(self.indices_neg):\n",
    "            self.used_indices_neg = []\n",
    "            np.random.shuffle(self.indices_neg)\n",
    "\n",
    "        indices_neg = self.indices_neg[len(self.used_indices_neg):len(self.used_indices_neg) + num_samples_1]\n",
    "        self.used_indices_neg.extend(indices_neg)\n",
    "        return indices_neg\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.indices) + len(self.indices_neg)) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param():\n",
    "    USER_NAME = 'JH'                  \n",
    "    SEED      = 9861\n",
    "    batch_size= 128\n",
    "    epoch     = 200\n",
    "    lr        = 0.0001\n",
    "    drop_rate = 0\n",
    "    hidden = {\n",
    "        'hidden' : []\n",
    "    }\n",
    "    return hidden, SEED, batch_size, lr, epoch, drop_rate\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "class C51DQNDense(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, N_ATOM, hidden_sizes):\n",
    "        super(C51DQNDense, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.N_ATOM = N_ATOM\n",
    "\n",
    "        self.layer = nn.ModuleList([\n",
    "            nn.Linear(in_features=state_dim, out_features=hidden_sizes[0]),\n",
    "            nn.BatchNorm1d(hidden_sizes[0]),\n",
    "            nn.ELU()\n",
    "        ])\n",
    "        \n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.layer.extend([\n",
    "                nn.Linear(in_features=hidden_sizes[i-1], out_features=hidden_sizes[i]),\n",
    "                nn.BatchNorm1d(hidden_sizes[i]),\n",
    "                nn.ELU()\n",
    "            ])\n",
    "\n",
    "        self.layer.append(nn.Linear(in_features=hidden_sizes[-1], out_features=action_dim * N_ATOM))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mb_size = x.size(0)\n",
    "\n",
    "        for layer in self.layer:\n",
    "            x = layer(x)\n",
    "    \n",
    "        action_value = F.softmax(x.view(mb_size, action_dim, N_ATOM), dim=2)\n",
    "        \n",
    "        return action_value\n",
    "    \n",
    "    def save(self, PATH):\n",
    "        torch.save(self,PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be optimized\n",
    "    hidden_sizes = {\n",
    "        'hidden': []\n",
    "    }\n",
    "    for idx in range(trial.suggest_int('num_layers', 2,5)):\n",
    "        hidden_sizes['hidden'].append(trial.suggest_int(f'h_{idx}', 10, 200))\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    lr = trial.suggest_categorical('lr', [0.000001, 0.00001, 0.00003, 0.00005, 0.0001])  # ,0.00001,0.00003,0.00005 ,0.0001,0.0003,\n",
    "    epoch_num = trial.suggest_categorical('epoch', [50])\n",
    "    gamma = trial.suggest_categorical('gamma', [0.99, 0.9, 0.8])\n",
    "    #drop_rate = trial.suggest_uniform('drop_rate', 0.0, 0.1)\n",
    "    update_interval = trial.suggest_int('update_interval', 2, 20)\n",
    "    #update_interval = trial.suggest_int('update_interval', 200, 300)\n",
    "\n",
    "    # Initialize your network with the suggested hyperparameters\n",
    "    print('============================================================')\n",
    "    print('state_dim: ', state_dim)\n",
    "    print('action_dim: ', action_dim)\n",
    "    print('learning_rate: ',lr)\n",
    "    print('hidden_sizes: ', hidden_sizes)\n",
    "    print('gamma: ', gamma)\n",
    "    #print('drop_rate: ', drop_rate)\n",
    "    print('batch_size: ', batch_size)\n",
    "    print('epoch_num: ', epoch_num)\n",
    "    print('update_interval: ', update_interval)\n",
    "    \n",
    "    network = C51OffDQNAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=lr, update_interval=update_interval, hidden_sizes=hidden_sizes['hidden'], gamma=gamma) #, drop_out=drop_rate\n",
    "    print('------------------------------------------------------------')\n",
    "    print(network.policy)\n",
    "    print('============================================================')\n",
    "    # Train the network and get validation loss\n",
    "    val_loss = train_and_evaluate(network=network, train_transition=train_transition, val_transition=val_transition, batch_size=batch_size,  epoch_num=epoch_num)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "V_MIN = -140.\n",
    "V_MAX = 140.\n",
    "N_ATOM = 51\n",
    "V_RANGE = np.linspace(V_MIN, V_MAX, N_ATOM)\n",
    "V_STEP = ((V_MAX-V_MIN)/(N_ATOM-1))\n",
    "\n",
    "PRED_PATH = f'./Model VM/vm_C51_pred_net_{date}.pt'\n",
    "TARGET_PATH = f'./Model VM/vm_C51_target_net_{date}.pt'\n",
    "RESULT_PATH = f'./Model VM/vm_C51_result_{date}.pt'\n",
    "\n",
    "class C51OffDQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes, update_interval, gamma, N_ATOM=51, learning_rate=0.0005):\n",
    "        #super().__init__(state_dim, action_dim)\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.name = 'C51OffDQNAgent'\n",
    "        self.batches_done = 0\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.target_update_steps = update_interval\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.N_ATOM = N_ATOM\n",
    "\n",
    "        print('Utilizing device {}'.format(self.device))\n",
    "        self.policy = C51DQNDense(state_dim=state_dim, action_dim=action_dim, N_ATOM=N_ATOM, hidden_sizes=hidden_sizes).to(self.device)\n",
    "        self.target = C51DQNDense(state_dim=state_dim, action_dim=action_dim, N_ATOM=N_ATOM, hidden_sizes=hidden_sizes).to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "\n",
    "        # discrete values\n",
    "        self.value_range = torch.FloatTensor(V_RANGE).to(self.device)\n",
    "\n",
    "    def save_model(self):\n",
    "        # save prediction network and target network\n",
    "        self.policy.save(PRED_PATH)\n",
    "        self.target.save(TARGET_PATH)\n",
    "\n",
    "    def load_model(self):\n",
    "        # load prediction network and target network\n",
    "        self.policy.load(PRED_PATH)\n",
    "        self.target.load(TARGET_PATH)\n",
    "\n",
    "    def act(self, state: np.ndarray) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            self.policy.eval()\n",
    "            state = torch.tensor(state).float().unsqueeze(0).to(self.device)\n",
    "            action = torch.argmax(self.policy(state))\n",
    "            return action.cpu().detach().numpy()\n",
    "\n",
    "    def train_batch(self, states, actions, rewards, next_states, terminals):\n",
    "        self.policy.train()\n",
    "\n",
    "        state = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        action = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        reward = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        new_state = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        done = torch.tensor(terminals, dtype=torch.long).to(self.device)\n",
    "\n",
    "        weight, b_idxes = np.ones_like(rewards), None\n",
    "\n",
    "        # action value distribution prediction\n",
    "        q_eval = self.policy(state) # (m, N_ACTIONS, N_ATOM) # (m, N_ACTIONS, N_ATOM) \n",
    "        mb_size = q_eval.size(0)\n",
    "        q_eval = torch.stack([q_eval[i].index_select(0, action[i]) for i in range(mb_size)]).squeeze(1) # (m, N_ATOM)\n",
    "        \n",
    "        # target distribution\n",
    "        q_target = np.zeros((mb_size, N_ATOM)) # (m, N_ATOM)\n",
    "        \n",
    "        # get next state value\n",
    "        q_next = self.target(new_state).detach() # (m, N_ACTIONS, N_ATOM)\n",
    "        # next value mean\n",
    "        q_next_mean = torch.sum(q_next * self.value_range.view(1, 1, -1), dim=2) # (m, N_ACTIONS)\n",
    "\n",
    "        \n",
    "        best_actions = q_next_mean.argmax(dim=1) # (m)\n",
    "        q_next = torch.stack([q_next[i].index_select(0, best_actions[i]) for i in range(mb_size)]).squeeze(1) \n",
    "        q_next = q_next.data.cpu().numpy() # (m, N_ATOM)\n",
    "        \n",
    "\n",
    "        # categorical projection\n",
    "        '''\n",
    "        next_v_range : (z_j) i.e. values of possible return, shape : (m, N_ATOM)\n",
    "        next_v_pos : relative position when offset of value is V_MIN, shape : (m, N_ATOM)\n",
    "        '''\n",
    "        # we vectorized the computation of support and position\n",
    "        reward_np = reward.data.cpu().numpy()\n",
    "        done_np = done.data.cpu().numpy()\n",
    "        value_range_np = self.value_range.data.cpu().numpy()\n",
    "\n",
    "        reward_np = np.expand_dims(reward_np, 1)\n",
    "        done_np = np.expand_dims((1. - done_np), 1)\n",
    "        #value_range_np = np.expand_dims(value_range_np, 0)\n",
    "\n",
    "        next_v_range = reward_np + (self.gamma * done_np * value_range_np).squeeze(1)\n",
    "\n",
    "        #next_v_range = np.expand_dims(reward.data.cpu().numpy(), 1) + gamma * np.expand_dims((1. - done.data.cpu().numpy()),1) * np.expand_dims(self.value_range.data.cpu().numpy(),0)\n",
    "        \n",
    "        next_v_pos = np.zeros_like(next_v_range)\n",
    "        #print('next_v_pos:', next_v_pos.shape)\n",
    "\n",
    "        # clip for categorical distribution\n",
    "        next_v_range = np.clip(next_v_range, V_MIN, V_MAX)\n",
    "        \n",
    "        # calc relative position of possible value\n",
    "        next_v_pos = (next_v_range - V_MIN)/ V_STEP\n",
    "        # get lower/upper bound of relative position\n",
    "        lb = np.floor(next_v_pos).astype(int)\n",
    "        ub = np.ceil(next_v_pos).astype(int)\n",
    "        # we didn't vectorize the computation of target assignment.\n",
    "        #print('lb: ', lb.shape)\n",
    "        #print('ub: ', ub.shape)\n",
    "        #print('mb_size: ', mb_size)\n",
    "        #print('N_ATOM: ', N_ATOM)\n",
    "        #print('q_next shape: ', q_next.shape)\n",
    "        for i in range(mb_size):\n",
    "            for j in range(N_ATOM):\n",
    "                # calc prob mass of relative position weighted with distance\n",
    "                q_target[i, lb[i,j]] += (q_next * (ub - next_v_pos))[i,j]\n",
    "                q_target[i, ub[i,j]] += (q_next * (next_v_pos - lb))[i,j]\n",
    "                \n",
    "        q_target = torch.FloatTensor(q_target).to(self.device) \n",
    "        \n",
    "        loss = q_target * ( - torch.log(q_eval + 1e-10)) # (m , N_ATOM)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        '''weight = torch.Tensor(weight).to(self.device) \n",
    "        loss = torch.mean(weight*loss)'''\n",
    "        \n",
    "        \n",
    "        # backprop loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        # Update target every n steps\n",
    "        if self.batches_done % self.target_update_steps == 0:\n",
    "            self.target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.batches_done += 1\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def validate_batch_loss(self, states, actions, rewards, next_states, terminals):\n",
    "        with torch.no_grad():\n",
    "            self.policy.eval()\n",
    "\n",
    "            state = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "            action = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "            reward = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "            new_state = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "            done = torch.tensor(terminals, dtype=torch.long).to(self.device)\n",
    "\n",
    "            weight, b_idxes = np.ones_like(rewards), None\n",
    "\n",
    "            # action value distribution prediction\n",
    "            q_eval = self.policy(state) # (m, N_ACTIONS, N_ATOM)\n",
    "            mb_size = q_eval.size(0)\n",
    "            q_eval = torch.stack([q_eval[i].index_select(0, action[i]) for i in range(mb_size)]).squeeze(1) # (m, N_ATOM)\n",
    "            \n",
    "            # target distribution\n",
    "            q_target = np.zeros((mb_size, N_ATOM)) # (m, N_ATOM)\n",
    "            \n",
    "            # get next state value\n",
    "            q_next = self.target(new_state).detach() # (m, N_ACTIONS, N_ATOM)\n",
    "            # next value mean\n",
    "            q_next_mean = torch.sum(q_next * self.value_range.view(1, 1, -1), dim=2) # (m, N_ACTIONS)\n",
    "            best_actions = q_next_mean.argmax(dim=1) # (m)\n",
    "            q_next = torch.stack([q_next[i].index_select(0, best_actions[i]) for i in range(mb_size)]).squeeze(1) \n",
    "            q_next = q_next.data.cpu().numpy() # (m, N_ATOM)\n",
    "\n",
    "            # categorical projection\n",
    "            '''\n",
    "            next_v_range : (z_j) i.e. values of possible return, shape : (m, N_ATOM)\n",
    "            next_v_pos : relative position when offset of value is V_MIN, shape : (m, N_ATOM)\n",
    "            '''\n",
    "            # we vectorized the computation of support and position\n",
    "            reward_np = reward.data.cpu().numpy()\n",
    "            done_np = done.data.cpu().numpy()\n",
    "            value_range_np = self.value_range.data.cpu().numpy()\n",
    "\n",
    "            reward_np = np.expand_dims(reward_np, 1)\n",
    "            done_np = np.expand_dims((1. - done_np), 1)\n",
    "            #value_range_np = np.expand_dims(value_range_np, 0)\n",
    "\n",
    "            next_v_range = reward_np + (self.gamma * done_np * value_range_np).squeeze(1)\n",
    "            #next_v_range = np.expand_dims(reward.data.cpu().numpy(), 1) + gamma * np.expand_dims((1. - done.data.cpu().numpy()),1) * np.expand_dims(self.value_range.data.cpu().numpy(),0)\n",
    "            next_v_pos = np.zeros_like(next_v_range)\n",
    "            \n",
    "            \n",
    "            # clip for categorical distribution\n",
    "            next_v_range = np.clip(next_v_range, V_MIN, V_MAX)\n",
    "            # calc relative position of possible value\n",
    "            next_v_pos = (next_v_range - V_MIN)/ V_STEP\n",
    "            # get lower/upper bound of relative position\n",
    "            lb = np.floor(next_v_pos).astype(int)\n",
    "            ub = np.ceil(next_v_pos).astype(int)\n",
    "            # we didn't vectorize the computation of target assignment.\n",
    "            for i in range(mb_size):\n",
    "                for j in range(N_ATOM):\n",
    "                    # calc prob mass of relative position weighted with distance\n",
    "                    q_target[i, lb[i,j]] += (q_next * (ub - next_v_pos))[i,j]\n",
    "                    q_target[i, ub[i,j]] += (q_next * (next_v_pos - lb))[i,j]\n",
    "                    \n",
    "            q_target = torch.FloatTensor(q_target).to(self.device) \n",
    "\n",
    "            loss = q_target * ( - torch.log(q_eval + 1e-8)) # (m , N_ATOM)\n",
    "            loss = torch.mean(loss)\n",
    "\n",
    "            '''weight = torch.Tensor(weight).to(self.device) \n",
    "            loss = torch.mean(weight*loss)'''\n",
    "\n",
    "            return loss.item()\n",
    "\n",
    "    def validate_batch_auroc(self, states, actions, label):\n",
    "        with torch.no_grad():\n",
    "            self.policy.eval()\n",
    "\n",
    "            state = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "            action = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "            label = torch.tensor(label, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            q_value = self.policy(state).detach() # (m, N_ACTIONS, N_ATOM)\n",
    "            q_value_mean = torch.sum(q_value * self.value_range.view(1, 1, -1), dim=2) # (m, N_ACTIONS)\n",
    "            q_max = torch.max((q_value_mean+100)/200, dim=1)  \n",
    "            q_max_ls = [[val.item()] for val in q_max[0]]\n",
    "            \n",
    "            q_max_array = np.array(q_max_ls)\n",
    "            \n",
    "            label_array = label.data.cpu().numpy()\n",
    "            \n",
    "            auroc = roc_auc_score(label_array, -q_max_array) #\n",
    "            auprc = average_precision_score(label_array, -q_max_array)\n",
    "\n",
    "            return auroc, auprc\n",
    "    \n",
    "    def plot_q_value_distribution(self, state):\n",
    "        with torch.no_grad():\n",
    "            self.policy.eval()\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            q_dist = self.policy(state_tensor).detach().data.cpu().numpy() # (m, N_ACTIONS, N_ATOM)\n",
    "            print('q_dist: ', q_dist.shape)\n",
    "\n",
    "            q_value = self.policy(state_tensor).detach() # (m, N_ACTIONS, N_ATOM)\n",
    "            q_value_mean = torch.sum(q_value * self.value_range.view(1, 1, -1), dim=2) # (m, N_ACTIONS)\n",
    "            #q_dist = q_dist.squeeze().detach().numpy()  # 배치 차원 제거하고 numpy 배열로 변환\n",
    "        \n",
    "        #actions = list(range(q_dist.shape[1]))\n",
    "        atoms_ls = np.linspace(V_MIN, V_MAX, 51)\n",
    "        for batch_num in range(q_dist.shape[0]):\n",
    "            for action_num in range(q_dist.shape[1]):\n",
    "                q_probabilities = q_dist[batch_num,action_num]\n",
    "                q_value_mean_in_batch = q_value_mean[batch_num]\n",
    "                plt.bar(atoms_ls, q_probabilities)\n",
    "                plt.xlabel('Atoms')\n",
    "                plt.ylabel('Prob')\n",
    "                plt.title(f'Q-value Distribution for State-Action {action_num}, Q value is {q_value_mean_in_batch[action_num]}')\n",
    "                plt.savefig(f'./Q_distribution graph/VM/Q-value Distribution for State-Action {action_num}')\n",
    "                plt.show()\n",
    "                print('------------------------------------------------')\n",
    "                print(f'Action {action_num} Q value: ', q_value_mean_in_batch[action_num])\n",
    "                print('================================================')\n",
    "\n",
    "    def print_q_value_distribution(self, state):\n",
    "        with torch.no_grad():\n",
    "            self.policy.eval()\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            q_dist = self.policy(state_tensor).detach().data.cpu().numpy() # (m, N_ACTIONS, N_ATOM)\n",
    "            \n",
    "            q_value = self.policy(state_tensor).detach() # (m, N_ACTIONS, N_ATOM)\n",
    "            q_value_mean = torch.sum(q_value * self.value_range.view(1, 1, -1), dim=2) # (m, N_ACTIONS)\n",
    "\n",
    "            return q_dist, q_value_mean\n",
    "\n",
    "\n",
    "    def print_policy(self) :\n",
    "        dict = {}\n",
    "        with torch.no_grad():\n",
    "            self.policy.eval()\n",
    "            for state in range(self.state_dim):\n",
    "                state_ls = [state]\n",
    "                state_one_hot = one_hot_encode(data=state_ls, num_classes=state_dim)\n",
    "                state_tensor = torch.tensor(state_one_hot, dtype=torch.float32)\n",
    "                q_value = network.policy(state_tensor)\n",
    "                q_value_mean = torch.sum(q_value * self.value_range.view(1, 1, -1), dim=2) # (m, N_ACTIONS)\n",
    "                best_actions = q_value_mean.argmax(dim=1)\n",
    "                action = best_actions.item()\n",
    "                dict[state] = action\n",
    "\n",
    "def train_and_evaluate(network, train_transition, val_transition, batch_size, epoch_num):\n",
    "    torch.manual_seed(220604)\n",
    "    np.random.seed(220604)\n",
    "    '''train_data_loader = DataLoader(dataset=train_transition,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                pin_memory=True)'''\n",
    "    sampler = CustomSampler(data=train_transition, batch_size=batch_size, ns=int(batch_size*0.05), target=[0.0, 10.0, 20.0, 30.0])\n",
    "    train_data_loader = DataLoader(train_transition, batch_sampler=sampler, num_workers=4)\n",
    "        \n",
    "    val_data_loader = DataLoader(dataset=val_transition,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            pin_memory=True)\n",
    "    # DataLoader 생성\n",
    "    total_loss = []\n",
    "    Total_num = 0\n",
    "    \n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_auroc = []\n",
    "    epoch_val_auprc = []\n",
    "    Total_num += 1\n",
    "    for epoch in tqdm(range(1, epoch_num+1)):  # 에폭 수 설정\n",
    "        batch_num = 0\n",
    "        batch_loss = []\n",
    "        #train\n",
    "        for i_batch, batch_data in enumerate(train_data_loader):\n",
    "            batch_num += 1\n",
    "            train_states, train_actions, train_rewards, train_next_states, train_terminal = batch_data\n",
    "            #print('train_states', train_states.shape)\n",
    "            loss = network.train_batch(train_states, train_actions, train_rewards, train_next_states, train_terminal)\n",
    "            #print(f'Batch {batch_num} Loss : {loss}')\n",
    "            #print('------------------------------------------------------------')\n",
    "            batch_loss.append(loss)\n",
    "\n",
    "        reward = []\n",
    "        q_value = []\n",
    "\n",
    "        val_batch_num = 0\n",
    "        val_batch_loss = []\n",
    "        val_batch_auroc = []\n",
    "        val_batch_auprc = []\n",
    "        for val_i_batch, val_batch_data in enumerate(val_data_loader):\n",
    "            val_batch_num += 1\n",
    "            val_states, val_actions, val_rewards, val_next_states, val_terminal = val_batch_data\n",
    "            #reward.append(val_rewards)\n",
    "            #network.policy(val_states), q_max\n",
    "            #q_value = []\n",
    "            #AUROC\n",
    "            val_loss = network.validate_batch_loss(val_states, val_actions, val_rewards, val_next_states, val_terminal) #\n",
    "            #print(f'Batch {batch_num} Loss : {loss}')\n",
    "            #print('------------------------------------------------------------')\n",
    "            val_batch_loss.append(val_loss)\n",
    "            #val_batch_auroc.append(val_auroc)\n",
    "        \n",
    "        for val_i_batch_auroc, val_batch_data_auroc in enumerate(total_val_transition):\n",
    "            val_batch_num += 1\n",
    "            val_states, val_actions, val_labels = val_batch_data_auroc\n",
    "            #AUROC\n",
    "            val_auroc, val_auprc = network.validate_batch_auroc(val_states, val_actions, val_labels) #\n",
    "            val_batch_auroc.append(val_auroc)\n",
    "            val_batch_auprc.append(val_auprc)\n",
    "\n",
    "        batch_loss_mean = np.mean(batch_loss)\n",
    "        batch_val_loss_mean = np.mean(val_batch_loss)\n",
    "        batch_val_auroc_mean = np.mean(val_batch_auroc)\n",
    "        batch_val_auprc_mean = np.mean(val_batch_auprc)\n",
    "        print(f'Epoch {epoch} loss : {batch_loss_mean}')\n",
    "        print(f'Epoch {epoch} val_loss : {batch_val_loss_mean}')\n",
    "        print(f'Epoch {epoch} val_auroc : {batch_val_auroc_mean}')\n",
    "        print(f'Epoch {epoch} val_auprc : {batch_val_auprc_mean}')\n",
    "        print('------------------------------------------------------------')\n",
    "        epoch_loss.append(batch_loss_mean)\n",
    "        epoch_val_loss.append(batch_val_loss_mean)\n",
    "        epoch_val_auroc.append(batch_val_auroc_mean)\n",
    "        epoch_val_auprc.append(batch_val_auprc_mean)\n",
    "    \n",
    "    y1 = epoch_val_loss\n",
    "    y2 = epoch_val_auroc\n",
    "    y3 = epoch_val_auprc\n",
    "    x = list(range(1,len(y1)+1))\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # 첫 번째 서브플롯\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(x, y1, marker='x', label='Epoch Val loss', color='limegreen', linestyle='-')\n",
    "    plt.title(\"Epoch Val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Val_loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 두 번째 서브플롯\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x, y2, marker='o', label='Epoch Val AUROC', color='blue', linestyle='-')\n",
    "    plt.title(\"Epoch Val AUROC\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Val_auroc\")\n",
    "    plt.legend()\n",
    "\n",
    "    # tp 번째 서브플롯\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(x, y3, marker='o', label='Epoch Val AUPRC', color='blue', linestyle='-')\n",
    "    plt.title(\"Epoch Val AUPRC\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Val_auprc\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #validataion loss 추가\n",
    "    epoch_loss_mean = np.mean(epoch_loss)\n",
    "    epoch_val_loss_mean = np.mean(epoch_val_loss)\n",
    "    epoch_val_auroc_mean = np.mean(epoch_val_auroc)\n",
    "    epoch_val_auprc_mean = np.mean(epoch_val_auprc)\n",
    "    print('Total val_loss_mean: ', epoch_val_loss_mean)\n",
    "    print('Total val_auroc_mean: ', epoch_val_auroc_mean)\n",
    "    print('Total val_auprc_mean: ', epoch_val_auprc_mean)\n",
    "    #print(f'Total {Total_num} loss : {epoch_loss_mean}')\n",
    "    #print(f'Total {Total_num} val_loss : {epoch_val_loss_mean}')\n",
    "    #print('============================================================')\n",
    "\n",
    "    return epoch_val_auroc_mean\n",
    "\n",
    "def train_and_evaluate_model(network, train_transition, val_transition, batch_size, epoch_num):\n",
    "    torch.manual_seed(220604)\n",
    "    np.random.seed(220604)\n",
    "    '''train_data_loader = DataLoader(dataset=train_transition,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                pin_memory=True)'''\n",
    "    sampler = CustomSampler(data=train_transition, batch_size=batch_size, ns=int(batch_size*0.05), target=[0.0, 10.0, 20.0, 30.0])\n",
    "    train_data_loader = DataLoader(train_transition, batch_sampler=sampler, num_workers=4)\n",
    "        \n",
    "    val_data_loader = DataLoader(dataset=val_transition,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            pin_memory=True)\n",
    "    # DataLoader 생성\n",
    "    total_loss = []\n",
    "    Total_num = 0\n",
    "    \n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_auroc = []\n",
    "    epoch_val_auprc = []\n",
    "    Total_num += 1\n",
    "    patience_value = 0\n",
    "    patience = 5\n",
    "    for epoch in tqdm(range(1, epoch_num+1)):  # 에폭 수 설정\n",
    "        batch_num = 0\n",
    "        batch_loss = []\n",
    "        #train\n",
    "        for i_batch, batch_data in enumerate(train_data_loader):\n",
    "            batch_num += 1\n",
    "            train_states, train_actions, train_rewards, train_next_states, train_terminal = batch_data\n",
    "            #print('train_states', train_states.shape)\n",
    "            loss = network.train_batch(train_states, train_actions, train_rewards, train_next_states, train_terminal)\n",
    "            #print(f'Batch {batch_num} Loss : {loss}')\n",
    "            #print('------------------------------------------------------------')\n",
    "            batch_loss.append(loss)\n",
    "\n",
    "        reward = []\n",
    "        q_value = []\n",
    "\n",
    "        val_batch_num = 0\n",
    "        val_batch_loss = []\n",
    "        val_batch_auroc = []\n",
    "        val_batch_auprc = []\n",
    "        for val_i_batch, val_batch_data in enumerate(val_data_loader):\n",
    "            val_batch_num += 1\n",
    "            val_states, val_actions, val_rewards, val_next_states, val_terminal = val_batch_data\n",
    "            #reward.append(val_rewards)\n",
    "            #network.policy(val_states), q_max\n",
    "            #q_value = []\n",
    "            #AUROC\n",
    "            val_loss = network.validate_batch_loss(val_states, val_actions, val_rewards, val_next_states, val_terminal) #\n",
    "            #print(f'Batch {batch_num} Loss : {loss}')\n",
    "            #print('------------------------------------------------------------')\n",
    "            val_batch_loss.append(val_loss)\n",
    "            #val_batch_auroc.append(val_auroc)\n",
    "        \n",
    "        for val_i_batch_auroc, val_batch_data_auroc in enumerate(total_val_transition):\n",
    "            val_batch_num += 1\n",
    "            val_states, val_actions, val_labels = val_batch_data_auroc\n",
    "            #AUROC\n",
    "            val_auroc, val_auprc = network.validate_batch_auroc(val_states, val_actions, val_labels) #\n",
    "            val_batch_auroc.append(val_auroc)\n",
    "            val_batch_auprc.append(val_auprc)\n",
    "\n",
    "        batch_loss_mean = np.mean(batch_loss)\n",
    "        batch_val_loss_mean = np.mean(val_batch_loss)\n",
    "        batch_val_auroc_mean = np.mean(val_batch_auroc)\n",
    "        batch_val_auprc_mean = np.mean(val_batch_auprc)\n",
    "        print(f'Epoch {epoch} loss : {batch_loss_mean}')\n",
    "        print(f'Epoch {epoch} val_loss : {batch_val_loss_mean}')\n",
    "        print(f'Epoch {epoch} val_auroc : {batch_val_auroc_mean}')\n",
    "        print(f'Epoch {epoch} val_auprc : {batch_val_auprc_mean}')\n",
    "        print('------------------------------------------------------------')\n",
    "        epoch_loss.append(batch_loss_mean)\n",
    "        epoch_val_loss.append(batch_val_loss_mean)\n",
    "        epoch_val_auroc.append(batch_val_auroc_mean)\n",
    "        epoch_val_auprc.append(batch_val_auprc_mean)\n",
    "\n",
    "        #network.save_model()\n",
    "        \n",
    "        if (len(epoch_val_loss) > 1) & (epoch_val_loss[-1] == min(epoch_val_loss)):\n",
    "            if (batch_val_auroc_mean>0.8) & (batch_val_auprc_mean>0.7) :\n",
    "                # save model\n",
    "                network.save_model()\n",
    "        \n",
    "        if epoch > 1:\n",
    "            if epoch_val_loss[-1] > epoch_val_loss[-2]:\n",
    "                patience_value += 1\n",
    "\n",
    "        if patience_value == patience :\n",
    "            print('------------------------------------------------')\n",
    "            print(f'epoch {epoch} End')\n",
    "            print('================================================')\n",
    "            patience_value = 0\n",
    "            break\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    y1 = epoch_val_loss\n",
    "    y2 = epoch_val_auroc\n",
    "    y3 = epoch_val_auprc\n",
    "    x = list(range(1,len(y1)+1))\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # 첫 번째 서브플롯\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(x, y1, marker='x', label='Epoch Val loss', color='limegreen', linestyle='-')\n",
    "    plt.title(\"Epoch Val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Val_loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 두 번째 서브플롯\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x, y2, marker='o', label='Epoch Val AUROC', color='blue', linestyle='-')\n",
    "    plt.title(\"Epoch Val AUROC\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Val_auroc\")\n",
    "    plt.legend()\n",
    "\n",
    "    # tp 번째 서브플롯\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(x, y3, marker='o', label='Epoch Val AUPRC', color='blue', linestyle='-')\n",
    "    plt.title(\"Epoch Val AUPRC\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Val_auprc\")\n",
    "    plt.legend()\n",
    "\n",
    "    #plt.tight_layout()  # 서브플롯 간의 간격 조정\n",
    "    plt.show()\n",
    "\n",
    "    #validataion loss 추가\n",
    "    epoch_loss_mean = np.mean(epoch_loss)\n",
    "    epoch_val_loss_mean = np.mean(epoch_val_loss)\n",
    "    epoch_val_auroc_mean = np.mean(epoch_val_auroc)\n",
    "    epoch_val_auprc_mean = np.mean(epoch_val_auprc)\n",
    "    print('Total val_loss_mean: ', epoch_val_loss_mean)\n",
    "    print('Total val_auroc_mean: ', epoch_val_auroc_mean)\n",
    "    print('Total val_auprc_mean: ', epoch_val_auprc_mean)\n",
    "    #print(f'Total {Total_num} loss : {epoch_loss_mean}')\n",
    "    #print(f'Total {Total_num} val_loss : {epoch_val_loss_mean}')\n",
    "    #print('============================================================')\n",
    "\n",
    "    return epoch_val_auroc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터셋 생성\n",
    "train_states, train_actions, train_rewards, train_next_states, train_terminal = make_transition(train_data)\n",
    "\n",
    "# PyTorch DataLoader로 데이터셋을 불러옵니다.\n",
    "train_transition = TensorDataset(torch.tensor(train_states, dtype=torch.float32),\n",
    "                        torch.tensor(train_actions, dtype=torch.long),  # action을 long으로 변환\n",
    "                        torch.tensor(train_rewards, dtype=torch.float32),\n",
    "                        torch.tensor(train_next_states, dtype=torch.float32),\n",
    "                        torch.tensor(train_terminal, dtype=torch.float32))\n",
    "\n",
    "# val 데이터셋 생성\n",
    "\n",
    "val_states, val_actions, val_rewards, val_next_states, val_terminal = make_transition(val_data)\n",
    "\n",
    "# PyTorch DataLoader로 데이터셋을 불러옵니다.\n",
    "val_transition = TensorDataset(torch.tensor(val_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_actions, dtype=torch.long),  # action을 long으로 변환\n",
    "                        torch.tensor(val_rewards, dtype=torch.float32),\n",
    "                        torch.tensor(val_next_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_terminal, dtype=torch.float32))\n",
    "\n",
    "#make_transition_test\n",
    "batch_size= len(val_data)\n",
    "\n",
    "s_col = [x for x in val_data.columns if x[:2]=='s:']\n",
    "a_col = [x for x in val_data.columns if x[:2]=='a:']\n",
    "r_col = [x for x in val_data.columns if x[:2]=='r:']\n",
    "label_col = [x for x in val_data.columns if x=='sh:Shock']\n",
    "t_col = [x for x in val_data.columns if x[:2]=='t:']\n",
    "dict = {}\n",
    "dict['traj'] = {}\n",
    "\n",
    "s = []\n",
    "a = []\n",
    "label = []\n",
    "\n",
    "for traj in tqdm(val_data.stay_id.unique()):\n",
    "    df_traj = val_data[val_data['stay_id'] == traj]\n",
    "    dict['traj'][traj] = {'s':[],'a':[],\n",
    "                        'label':[]}\n",
    "    dict['traj'][traj]['s'] = df_traj[s_col].values.tolist()\n",
    "    dict['traj'][traj]['a'] = df_traj[a_col].values.tolist()\n",
    "    dict['traj'][traj]['label'] = df_traj[label_col].values.tolist()\n",
    "    \n",
    "    for step in range(len(df_traj)):\n",
    "        s.append(dict['traj'][traj]['s'][step])\n",
    "        a.append(dict['traj'][traj]['a'][step])\n",
    "        label.append(dict['traj'][traj]['label'][step])\n",
    "\n",
    "s = torch.FloatTensor(np.float32(s))\n",
    "a = torch.LongTensor(np.int64(a))\n",
    "label = torch.LongTensor(np.int64(label))\n",
    "\n",
    "Dataset = TensorDataset(s,a,label)\n",
    "                        \n",
    "total_val_transition = DataLoader(Dataset,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']= '1'\n",
    "n_gpu             = 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print(device)\n",
    "\n",
    "# Set parameters\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction=\"maximize\")\n",
    "study.optimize(objective, n_trials = 3)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "batch_size = 256\n",
    "update_interval = 6\n",
    "learning_rate = 0.0001\n",
    "epoch_num = 200\n",
    "\n",
    "network = C51OffDQNAgent(state_dim=state_dim, action_dim=action_dim, learning_rate=learning_rate, update_interval=update_interval, hidden_sizes=[167, 196, 121, 57], gamma=0.8)\n",
    "print(network.policy)\n",
    "\n",
    "result = train_and_evaluate_model(network, train_transition, val_transition, batch_size, epoch_num)\n",
    "\n",
    "'''\n",
    "Param 1:\n",
    "state_dim:  30\n",
    "action_dim:  6\n",
    "learning_rate:  0.0001\n",
    "hidden_sizes:  {'hidden': [167, 196, 121, 57]}\n",
    "gamma:  0.8\n",
    "batch_size:  128\n",
    "epoch_num:  50\n",
    "update_interval:  6\n",
    "Utilizing device cuda:1\n",
    "--------------------------------------------------\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_states, val_actions, val_rewards, val_next_states, val_terminal = make_transition(val_data)\n",
    "\n",
    "# PyTorch DataLoader로 데이터셋을 불러옵니다.\n",
    "val_transition = TensorDataset(torch.tensor(val_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_actions, dtype=torch.long),  # action을 long으로 변환\n",
    "                        torch.tensor(val_rewards, dtype=torch.float32),\n",
    "                        torch.tensor(val_next_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_terminal, dtype=torch.float32))\n",
    "\n",
    "#make_transition_test\n",
    "batch_size= 1\n",
    "\n",
    "stay_id_col = [x for x in val_data.columns if x=='stay_id']\n",
    "s_col = [x for x in val_data.columns if x[:2]=='s:']\n",
    "a_col = [x for x in val_data.columns if x[:2]=='a:']\n",
    "r_col = [x for x in val_data.columns if x[:2]=='r:']\n",
    "label_col = [x for x in val_data.columns if x=='sh:Shock']\n",
    "t_col = [x for x in val_data.columns if x[:2]=='t:']\n",
    "dict = {}\n",
    "dict['traj'] = {}\n",
    "\n",
    "id = []\n",
    "s = []\n",
    "a = []\n",
    "label = []\n",
    "\n",
    "for traj in tqdm(np.array([30004798])):\n",
    "    df_traj = val_data[val_data['stay_id'] == traj]\n",
    "    dict['traj'][traj] = {'id':[],'s':[],'a':[],'label':[]}\n",
    "    dict['traj'][traj]['id'] = df_traj[stay_id_col].values.tolist()\n",
    "    dict['traj'][traj]['s'] = df_traj[s_col].values.tolist()\n",
    "    dict['traj'][traj]['a'] = df_traj[a_col].values.tolist()\n",
    "    dict['traj'][traj]['label'] = df_traj[label_col].values.tolist()\n",
    "    \n",
    "    for step in range(len(df_traj)):\n",
    "        id.append(dict['traj'][traj]['id'][step])\n",
    "        s.append(dict['traj'][traj]['s'][step])\n",
    "        a.append(dict['traj'][traj]['a'][step])\n",
    "        label.append(dict['traj'][traj]['label'][step])\n",
    "\n",
    "id = torch.FloatTensor(np.int64(id))\n",
    "s = torch.FloatTensor(np.float32(s))\n",
    "a = torch.LongTensor(np.int64(a))\n",
    "label = torch.LongTensor(np.int64(label))\n",
    "\n",
    "Dataset = TensorDataset(s,a,label,id)\n",
    "                        \n",
    "total_val_transition_for_plot_alive = DataLoader(Dataset,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val 데이터셋 생성\n",
    "val_states, val_actions, val_rewards, val_next_states, val_terminal = make_transition(val_data)\n",
    "\n",
    "# PyTorch DataLoader로 데이터셋을 불러옵니다.\n",
    "val_transition = TensorDataset(torch.tensor(val_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_actions, dtype=torch.long),  # action을 long으로 변환\n",
    "                        torch.tensor(val_rewards, dtype=torch.float32),\n",
    "                        torch.tensor(val_next_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_terminal, dtype=torch.float32))\n",
    "\n",
    "#make_transition_test\n",
    "batch_size= 1\n",
    "\n",
    "stay_id_col = [x for x in val_data.columns if x=='stay_id']\n",
    "s_col = [x for x in val_data.columns if x[:2]=='s:']\n",
    "a_col = [x for x in val_data.columns if x[:2]=='a:']\n",
    "r_col = [x for x in val_data.columns if x[:2]=='r:']\n",
    "label_col = [x for x in val_data.columns if x=='sh:Shock']\n",
    "t_col = [x for x in val_data.columns if x[:2]=='t:']\n",
    "dict = {}\n",
    "dict['traj'] = {}\n",
    "\n",
    "id = []\n",
    "s = []\n",
    "a = []\n",
    "label = []\n",
    "\n",
    "for traj in tqdm(np.array([30025720])):\n",
    "    df_traj = val_data[val_data['stay_id'] == traj]\n",
    "    dict['traj'][traj] = {'id':[],'s':[],'a':[],'label':[]}\n",
    "    dict['traj'][traj]['id'] = df_traj[stay_id_col].values.tolist()\n",
    "    dict['traj'][traj]['s'] = df_traj[s_col].values.tolist()\n",
    "    dict['traj'][traj]['a'] = df_traj[a_col].values.tolist()\n",
    "    dict['traj'][traj]['label'] = df_traj[label_col].values.tolist()\n",
    "    \n",
    "    for step in range(len(df_traj)):\n",
    "        id.append(dict['traj'][traj]['id'][step])\n",
    "        s.append(dict['traj'][traj]['s'][step])\n",
    "        a.append(dict['traj'][traj]['a'][step])\n",
    "        label.append(dict['traj'][traj]['label'][step])\n",
    "\n",
    "id = torch.FloatTensor(np.int64(id))\n",
    "s = torch.FloatTensor(np.float32(s))\n",
    "a = torch.LongTensor(np.int64(a))\n",
    "label = torch.LongTensor(np.int64(label))\n",
    "\n",
    "Dataset = TensorDataset(s,a,label,id)\n",
    "                        \n",
    "total_val_transition_for_plot_dead = DataLoader(Dataset,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val 데이터셋 생성\n",
    "val_states, val_actions, val_rewards, val_next_states, val_terminal = make_transition(val_data)\n",
    "\n",
    "# PyTorch DataLoader로 데이터셋을 불러옵니다.\n",
    "val_transition = TensorDataset(torch.tensor(val_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_actions, dtype=torch.long),  # action을 long으로 변환\n",
    "                        torch.tensor(val_rewards, dtype=torch.float32),\n",
    "                        torch.tensor(val_next_states, dtype=torch.float32),\n",
    "                        torch.tensor(val_terminal, dtype=torch.float32))\n",
    "\n",
    "#make_transition_test\n",
    "batch_size= 1\n",
    "\n",
    "stay_id_col = [x for x in val_data.columns if x=='stay_id']\n",
    "s_col = [x for x in val_data.columns if x[:2]=='s:']\n",
    "a_col = [x for x in val_data.columns if x[:2]=='a:']\n",
    "r_col = [x for x in val_data.columns if x[:2]=='r:']\n",
    "label_col = [x for x in val_data.columns if x=='sh:Shock']\n",
    "t_col = [x for x in val_data.columns if x[:2]=='t:']\n",
    "dict = {}\n",
    "dict['traj'] = {}\n",
    "\n",
    "id = []\n",
    "s = []\n",
    "a = []\n",
    "label = []\n",
    "\n",
    "for traj in tqdm(val_data.stay_id.unique()):\n",
    "    df_traj = val_data[val_data['stay_id'] == traj]\n",
    "    dict['traj'][traj] = {'id':[],'s':[],'a':[],'label':[]}\n",
    "    dict['traj'][traj]['id'] = df_traj[stay_id_col].values.tolist()\n",
    "    dict['traj'][traj]['s'] = df_traj[s_col].values.tolist()\n",
    "    dict['traj'][traj]['a'] = df_traj[a_col].values.tolist()\n",
    "    dict['traj'][traj]['label'] = df_traj[label_col].values.tolist()\n",
    "    \n",
    "    for step in range(len(df_traj)):\n",
    "        id.append(dict['traj'][traj]['id'][step])\n",
    "        s.append(dict['traj'][traj]['s'][step])\n",
    "        a.append(dict['traj'][traj]['a'][step])\n",
    "        label.append(dict['traj'][traj]['label'][step])\n",
    "\n",
    "id = torch.FloatTensor(np.int64(id))\n",
    "s = torch.FloatTensor(np.float32(s))\n",
    "a = torch.LongTensor(np.int64(a))\n",
    "label = torch.LongTensor(np.int64(label))\n",
    "\n",
    "Dataset = TensorDataset(s,a,label,id)\n",
    "                        \n",
    "total_val_transition_for_plot = DataLoader(Dataset,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alive stay_id 찾아서 plot 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_num = 0\n",
    "val_batch_auroc = []\n",
    "val_data_df = val_data.copy()\n",
    "val_data_df['Action by RL max'] = None\n",
    "val_data_df['Action by RL over mean'] = None\n",
    "transition = 1\n",
    "for val_i_batch_auroc, val_batch_data_auroc in enumerate(total_val_transition_for_plot_alive):\n",
    "    print(f'Transition {transition}')\n",
    "    val_states, val_actions, val_labels, val_stay_id = val_batch_data_auroc\n",
    "    action = network.plot_q_value_distribution(state=val_states)\n",
    "    transition += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dead stay_id 찾아서 plot 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_num = 0\n",
    "val_batch_auroc = []\n",
    "val_data_df = val_data.copy()\n",
    "val_data_df['Action by RL max'] = None\n",
    "val_data_df['Action by RL over mean'] = None\n",
    "transition = 1\n",
    "for val_i_batch_auroc, val_batch_data_auroc in enumerate(total_val_transition_for_plot_dead):\n",
    "    print(f'Transition {transition}')\n",
    "    val_states, val_actions, val_labels, val_stay_id = val_batch_data_auroc\n",
    "    action = network.plot_q_value_distribution(state=val_states)\n",
    "    transition += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stay_id 찾아서 plot 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''index_num = 0\n",
    "val_batch_auroc = []\n",
    "val_data_df = val_data.copy()\n",
    "val_data_df['Action by RL max'] = None\n",
    "val_data_df['Action by RL over mean'] = None\n",
    "transition = 1\n",
    "for val_i_batch_auroc, val_batch_data_auroc in enumerate(total_val_transition_for_plot):\n",
    "    print(f'Transition {transition}')\n",
    "    val_states, val_actions, val_labels = val_batch_data_auroc\n",
    "    action = network.plot_q_value_distribution(state=val_states)\n",
    "    transition += 1\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''index_num = 0\n",
    "val_batch_auroc = []\n",
    "val_data_df = val_data.copy()\n",
    "val_data_df['Action by RL max'] = None\n",
    "val_data_df['Action by RL over mean'] = None'''\n",
    "transition = 1\n",
    "for val_i_batch_auroc, val_batch_data_auroc in enumerate(total_val_transition_for_plot):\n",
    "    print(f'Transition {transition}')\n",
    "    val_states, val_actions, val_labels = val_batch_data_auroc\n",
    "    q_dist, q_value_mean = network.print_q_value_distribution(state=val_states)\n",
    "\n",
    "    print('Q value distribution shape: ', q_dist.shape)\n",
    "    print('Q value distribution type: ', type(q_dist))\n",
    "    print('Q value distributions: ', q_dist)\n",
    "    print('-----------------------------------------------------')\n",
    "    print('Q value mean shape: ', q_value_mean.shape)\n",
    "    print('Q value mean type: ', type(q_value_mean))\n",
    "    print('Q value mean: ', q_value_mean)\n",
    "    print('=====================================================')\n",
    "\n",
    "    transition += 1\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KUxPITTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
