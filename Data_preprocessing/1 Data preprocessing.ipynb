{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "pd.set_option('display.max_seq_items', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = pd.read_csv('./admissions.csv')\n",
    "admissions = admissions[['hadm_id','deathtime','hospital_expire_flag']]\n",
    "admissions['deathtime'] = admissions['deathtime'].fillna(0)\n",
    "dead_index = admissions[(admissions['deathtime']!=0) & (admissions['hospital_expire_flag']==1)].index\n",
    "admissions['Death'] = 0\n",
    "admissions['Death'][dead_index] = 1\n",
    "\n",
    "death = admissions[['hadm_id','Death']]\n",
    "\n",
    "patients = pd.read_csv('./patients.csv')\n",
    "icustays = pd.read_csv('./icustays.csv')\n",
    "patients = patients[['subject_id','gender','anchor_age']]\n",
    "\n",
    "vital_lab = pd.read_csv('./10min vital lab Dataset.csv')\n",
    "Ketamine = pd.read_csv('./10min Ketamine.csv')\n",
    "Propofol = pd.read_csv('./10min Propofol.csv')\n",
    "Midazolam = pd.read_csv('./10min Midazolam.csv')\n",
    "Fentanyl = pd.read_csv('./10min Fentanyl.csv')\n",
    "fluids = pd.read_csv('./10min Fluids.csv')\n",
    "urine = pd.read_csv('./10min Urine output.csv')\n",
    "LR_NS_Pl = pd.read_csv('./10min LR_NS_TR_AL.csv')\n",
    "vasopressors = pd.read_csv('./10min Vasopressors.csv')\n",
    "transfusion = pd.read_csv('./10min transfusion.csv')\n",
    "transfusion.rename(columns={'tranfusion':'Transfusion'}, inplace=True)\n",
    "Epinephrine = pd.read_csv('./10min Epinephrine.csv')\n",
    "Norepinephrine = pd.read_csv('./10min Norepinephrine.csv')\n",
    "Phenylephrine = pd.read_csv('./10min Phenylephrine.csv')\n",
    "Vasopressin = pd.read_csv('./10min Vasopressin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vital_lab['stay_id'].unique().shape)\n",
    "print(Ketamine['stay_id'].unique().shape)\n",
    "print(Propofol['stay_id'].unique().shape)\n",
    "print(Midazolam['stay_id'].unique().shape)\n",
    "print(Fentanyl['stay_id'].unique().shape)\n",
    "print(fluids['stay_id'].unique().shape)\n",
    "print(urine['stay_id'].unique().shape)\n",
    "print(LR_NS_Pl['stay_id'].unique().shape)\n",
    "print(vasopressors['stay_id'].unique().shape)\n",
    "print(transfusion['stay_id'].unique().shape)\n",
    "print(Epinephrine['stay_id'].unique().shape)\n",
    "print(Norepinephrine['stay_id'].unique().shape)\n",
    "print(Phenylephrine['stay_id'].unique().shape)\n",
    "print(Vasopressin['stay_id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icustays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.merge(vital_lab, icustays, on=['subject_id', 'hadm_id','stay_id'], how='left')\n",
    "main = pd.merge(main, LR_NS_Pl, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, vasopressors, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "\n",
    "main = pd.merge(main, Epinephrine, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Norepinephrine, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Phenylephrine, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Vasopressin, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, transfusion, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Ketamine, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Propofol, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Midazolam, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, Fentanyl, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, fluids, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "main = pd.merge(main, urine, on=['subject_id', 'hadm_id','stay_id', 'time'], how='left')\n",
    "\n",
    "\n",
    "main = pd.merge(main, patients, on=['subject_id'], how='left')\n",
    "\n",
    "main = pd.merge(main, death, on=['hadm_id'], how='left')\n",
    "\n",
    "#main = main[main['LOS']>1].reset_index(drop=True)\n",
    "\n",
    "main['stay_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_height(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    height_mean = x['Height'].mean()\n",
    "\n",
    "    x['Height'] = height_mean\n",
    "\n",
    "    return x\n",
    "\n",
    "main = main.groupby('stay_id').apply(fillna_height)\n",
    "main.reset_index(drop=True, inplace=True)\n",
    "'''\n",
    "male_index = main[main['gender']=='M'].index\n",
    "female_index = main[main['gender']=='F'].index\n",
    "\n",
    "main['Ideal Weight'] = 0\n",
    "main['Ideal Weight'][male_index] = 50 + 2.3*(main['Height'][male_index]/2.54)/5\n",
    "main['Ideal Weight'][female_index] = 45.5 + 2.3*(main['Height'][female_index]/2.54)/5\n",
    "\n",
    "main = main.sort_values(by=['subject_id','hadm_id','stay_id', 'time'], ascending=[True,True,True,True])\n",
    "main.reset_index(drop=True, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation: Forward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['Actual Troponin test'] = 0 \n",
    "actual_trop_index = main[main['Troponin'].notna()].index\n",
    "main['Actual Troponin test'][actual_trop_index] = 1\n",
    "\n",
    "main['Actual Creatinine test'] = 0 \n",
    "actual_creatinine_index = main[main['Creatinine'].notna()].index\n",
    "main['Actual Creatinine test'][actual_creatinine_index] = 1\n",
    "\n",
    "main['Actual Lactate test'] = 0\n",
    "actual_Lactate_index = main[main['Lactate'].notna()].index\n",
    "main['Actual Lactate test'][actual_Lactate_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['IBlood pressure systolic'] = main['IBlood pressure systolic'].fillna(-1)\n",
    "main['IBlood pressure diastolic'] = main['IBlood pressure diastolic'].fillna(-1)\n",
    "main['IV fluids'] = main['IV fluids'].fillna(0)\n",
    "main['Vasopressors'] = main['Vasopressors'].fillna(0)\n",
    "main['Transfusion'] = main['Transfusion'].fillna(0)\n",
    "\n",
    "main['Epinephrine'] = main['Epinephrine'].fillna(0)\n",
    "main['Norepinephrine'] = main['Norepinephrine'].fillna(0)\n",
    "main['Phenylephrine'] = main['Phenylephrine'].fillna(0)\n",
    "main['Vasopressin'] = main['Vasopressin'].fillna(0)\n",
    "main['Ketamine'] = main['Ketamine'].fillna(0)\n",
    "main['Propofol'] = main['Propofol'].fillna(0)\n",
    "main['Midazolam'] = main['Midazolam'].fillna(0)\n",
    "main['Fentanyl'] = main['Fentanyl'].fillna(0)\n",
    "main['Urine Output'] = main['Urine Output'].fillna(0)\n",
    "main['LR_NS'] = main['LR_NS'].fillna(0)\n",
    "\n",
    "\n",
    "main = main[['subject_id', 'hadm_id','stay_id', 'time', 'intime', 'outtime',\n",
    "             'Death', 'gender', 'anchor_age', 'Weight', 'Height', \n",
    "             'Heart rate', 'NIBlood pressure systolic', 'NIBlood pressure diastolic', 'IBlood pressure systolic', 'IBlood pressure diastolic', 'Respiratory rate', 'SpO2', 'Temperature', \n",
    "             'Hemoglobin', 'Creatinine', 'Troponin', 'Lactate', \n",
    "             'Ketamine', 'Propofol', 'Midazolam', 'Fentanyl', \n",
    "             'IV fluids', 'Urine Output',\n",
    "             'LR_NS', 'Transfusion', 'Vasopressors', 'Epinephrine', 'Norepinephrine', 'Phenylephrine', 'Vasopressin',\n",
    "             'Actual Troponin test', 'Actual Creatinine test', 'Actual Lactate test'\n",
    "             ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간격 설정 보간\n",
    "def imputation(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #interval_backward = 6*6\n",
    "\n",
    "    x = x.fillna(method='ffill')\n",
    "\n",
    "    #x.loc[0:interval_backward] = x.loc[0:interval_backward].fillna(method='bfill')\n",
    "\n",
    "    return x\n",
    "\n",
    "imputated_data = main.groupby('stay_id').apply(imputation)\n",
    "main = 0\n",
    "imputated_data.reset_index(drop=True, inplace=True)\n",
    "print(imputated_data['stay_id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputated_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude Nan data in crucial feature(for labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bye_bye(x): \n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    column_list = ['NIBlood pressure systolic', 'NIBlood pressure diastolic']\n",
    "    \n",
    "    has_nan = x[column_list].isna().any(axis=1).any()\n",
    "\n",
    "    if has_nan == False:\n",
    "        return x\n",
    "    \n",
    "    else :\n",
    "        df_notna = x.dropna(subset=column_list)\n",
    "        df_notna.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df_isna = x[x[column_list].isnull().any(axis=1)]\n",
    "\n",
    "        if len(df_notna)!=0:\n",
    "            if df_notna['Actual Troponin test'][0]==0:\n",
    "                df_notna['Actual Troponin test'][0] = df_isna['Actual Troponin test'].max()\n",
    "\n",
    "            if df_notna['Actual Creatinine test'][0]==0: \n",
    "                df_notna['Actual Creatinine test'][0] = df_isna['Actual Creatinine test'].max()\n",
    "\n",
    "            if df_notna['Actual Lactate test'][0]==0:\n",
    "                df_notna['Actual Lactate test'][0] = df_isna['Actual Lactate test'].max()\n",
    "\n",
    "            return df_notna\n",
    "\n",
    "cleansed_data = imputated_data.groupby('stay_id').apply(Bye_bye)\n",
    "print(cleansed_data.shape)\n",
    "#imputated_data = 0\n",
    "cleansed_data.reset_index(drop=True, inplace=True)\n",
    "print(cleansed_data['stay_id'].unique().shape)\n",
    "\n",
    "cleansed_data.to_csv('./10min cleansed Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "pd.set_option('display.max_seq_items', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./10min cleansed Dataset.csv')\n",
    "\n",
    "print(data['stay_id'].unique().shape)\n",
    "nan_columns = data.columns[data.isnull().any()].tolist()\n",
    "print(\"Columns with NaN values :\", nan_columns)\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Invasive and Invasive blood pressure\n",
    "#Check Flash value\n",
    "flash_condition1 = (data['IBlood pressure systolic']*0.9 <= data['IBlood pressure diastolic'])\n",
    "flash_condition2 = (data['IBlood pressure systolic']!=-1)\n",
    "flash_condition3 = (data['IBlood pressure diastolic']!=-1)\n",
    "flash_index = data[flash_condition1 & flash_condition2 & flash_condition3].index\n",
    "data['IBlood pressure systolic'][flash_index] = -1\n",
    "data['IBlood pressure diastolic'][flash_index] = -1\n",
    "\n",
    "#handle invasive and noninvasive\n",
    "invasive_sbp_index = data[data['IBlood pressure systolic']!=-1].index\n",
    "invasive_dbp_index = data[data['IBlood pressure diastolic']!=-1].index\n",
    "\n",
    "data['SBP type'] = 'NI'\n",
    "data['DBP type'] = 'NI'\n",
    "data['NIBlood pressure systolic'][invasive_sbp_index] = data['IBlood pressure systolic'][invasive_sbp_index]\n",
    "data['NIBlood pressure diastolic'][invasive_dbp_index] = data['IBlood pressure diastolic'][invasive_dbp_index]\n",
    "data['SBP type'][invasive_sbp_index] = 'I'\n",
    "data['DBP type'][invasive_dbp_index] = 'I'\n",
    "\n",
    "data.drop(columns=['IBlood pressure systolic', 'IBlood pressure diastolic'], inplace=True)\n",
    "data.rename(columns={'NIBlood pressure systolic':'Blood pressure systolic'}, inplace=True)\n",
    "data.rename(columns={'NIBlood pressure diastolic':'Blood pressure diastolic'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBP\n",
    "data['Blood pressure mean'] = data['Blood pressure systolic']+(data['Blood pressure diastolic']-data['Blood pressure systolic'])/3\n",
    "\n",
    "data.to_csv('./10min Dataset for shock labeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data preprocessed 4/10min Dataset for shock labeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_peri_shock_weak(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    x['peri_Shock'] = 0\n",
    "\n",
    "    for p in range(len(x)):\n",
    "        if ((x['Blood pressure mean'][p]>65) & (x['Blood pressure mean'][p]<75)):               \n",
    "            x['peri_Shock'][p] = 1\n",
    "\n",
    "    return x\n",
    "\n",
    "def labeling_shock_weak(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    x['Shock'] = 0\n",
    "    x['Ambiguous Shock'] = 0\n",
    "\n",
    "    for p in range(len(x)):\n",
    "        if not math.isnan(x['Lactate'][p]):\n",
    "            if ((x['Blood pressure mean'][p]<=65) & (x['Lactate'][p]>=2)) | (x['Vasopressors'][p] > 0):               \n",
    "                x['Shock'][p] = 1\n",
    "        else:\n",
    "            if x['Vasopressors'][p] > 0 :\n",
    "                x['Shock'][p] = 1\n",
    "            elif x['Blood pressure mean'][p]<=65:               \n",
    "                x['Ambiguous Shock'][p] = 1 \n",
    "\n",
    "    return x\n",
    "\n",
    "def num_shock(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    new_df = pd.DataFrame(index=[[0]], columns=['stay_id','Shock','Death','Shock and Death','Ambiguous Shock'])\n",
    "\n",
    "    new_df['stay_id'][0] = x['stay_id'].max()\n",
    "    new_df['Shock'][0] = x['Shock'].max()\n",
    "    new_df['Ambiguous Shock'][0] = x['Shock'].max()\n",
    "    new_df['Death'][0] = x['Death'].max()\n",
    "    if (new_df['Shock'][0]==1) & (new_df['Death'][0]==1) :\n",
    "        new_df['Shock and Death'][0] = 1\n",
    "    else :\n",
    "        new_df['Shock and Death'][0] = 0\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result0 = data.groupby('stay_id').apply(labeling_peri_shock_weak)\n",
    "result0.reset_index(drop=True, inplace=True)\n",
    "print(result0['stay_id'].unique().shape)\n",
    "print(result0.shape)\n",
    "\n",
    "result1 = result0.groupby('stay_id').apply(labeling_shock_weak)\n",
    "result1.reset_index(drop=True, inplace=True)\n",
    "print(result1['stay_id'].unique().shape)\n",
    "print(result1.shape)\n",
    "\n",
    "result2 = result1.groupby('stay_id').apply(num_shock)\n",
    "shock_sum = result2['Shock'].sum()\n",
    "ambiguous_shock_sum = result2['Ambiguous Shock'].sum()\n",
    "shock_death_sum = result2['Shock and Death'].sum()\n",
    "\n",
    "print(result0['stay_id'].unique().shape)\n",
    "print(result1['stay_id'].unique().shape)\n",
    "print(shock_sum)\n",
    "print(ambiguous_shock_sum)\n",
    "print(shock_death_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.to_csv('./10min Shock Dataset labeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1['peri_Shock'].unique())\n",
    "print(result1['Shock'].unique())\n",
    "print(result1['Ambiguous Shock'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = pd.read_csv('./10min Shock Dataset labeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shock_patient(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    if x['Shock'].max() == 1:\n",
    "        return x\n",
    "    \n",
    "result3 = result1.groupby('stay_id').apply(shock_patient)\n",
    "result3.reset_index(drop=True, inplace=True)\n",
    "print(result3['stay_id'].unique().shape)\n",
    "print(result3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3['time'] = pd.to_datetime(result3['time'])\n",
    "result3['intime'] = pd.to_datetime(result3['intime'])\n",
    "result3['outtime'] = pd.to_datetime(result3['outtime'])\n",
    "\n",
    "result3['Shock index'] = result3['Heart rate']/result3['Blood pressure systolic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling_func_1hr(x):\n",
    "    mean_name = ['Weight', 'Height', 'Heart rate', 'Blood pressure systolic', 'Blood pressure diastolic', 'Blood pressure mean', 'Respiratory rate', 'SpO2', 'Temperature', 'Shock index',\n",
    "                    'Hemoglobin', 'Creatinine', 'Troponin', 'Lactate']\n",
    "    sum_name = ['Propofol', 'Midazolam', 'Fentanyl', 'IV fluids', 'Urine Output', 'LR_NS', 'Transfusion', 'Shock', 'Ambiguous Shock', 'peri_Shock']\n",
    "    max_name = ['Ketamine','Vasopressors','Epinephrine','Norepinephrine','Phenylephrine','Vasopressin', 'Actual Troponin test', 'Actual Creatinine test', 'Actual Lactate test']\n",
    "\n",
    "    column = ['subject_id','hadm_id','stay_id', 'time', 'Death', 'gender', 'anchor_age'] + mean_name + sum_name + max_name\n",
    "\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    date_range = pd.date_range(x['intime'][0], x['outtime'][0], freq='1H', inclusive='right')\n",
    "    new_df = pd.DataFrame(index=date_range, columns=column)\n",
    "    \n",
    "    new_df['subject_id'] = x['subject_id'][0]\n",
    "    new_df['hadm_id'] = x['hadm_id'][0]\n",
    "    new_df['stay_id'] = x['stay_id'][0]\n",
    "    new_df['time'] = new_df.index\n",
    "    new_df['Death'] = x['Death'][0]\n",
    "    new_df['gender'] = x['gender'][0]\n",
    "    new_df['anchor_age'] = x['anchor_age'][0]\n",
    "    \n",
    "    for i in range(len(date_range)):\n",
    "        if i == 0:\n",
    "            condition1 = date_range[i]>=x['time']\n",
    "            sa = x[condition1]\n",
    "            for p in mean_name:\n",
    "                new_df[p][i] = sa[p].mean()\n",
    "        else :\n",
    "            condition2 = date_range[i-1]<x['time']\n",
    "            condition3 = date_range[i]>=x['time']\n",
    "            sa = x[(condition2)&(condition3)]\n",
    "\n",
    "            for p in mean_name:\n",
    "                new_df[p][i] = sa[p].mean()\n",
    "\n",
    "    for i in range(len(date_range)):\n",
    "        if i == 0:\n",
    "            condition1 = date_range[i]>=x['time']\n",
    "            sa = x[condition1]\n",
    "            for p in sum_name:\n",
    "                new_df[p][i] = sa[p].sum()\n",
    "        else :\n",
    "            condition2 = date_range[i-1]<x['time']\n",
    "            condition3 = date_range[i]>=x['time']\n",
    "            sa = x[(condition2)&(condition3)]\n",
    "\n",
    "            for p in sum_name:\n",
    "                new_df[p][i] = sa[p].sum()\n",
    "\n",
    "    for i in range(len(date_range)):\n",
    "        if i == 0:\n",
    "            condition1 = date_range[i]>=x['time']\n",
    "            sa = x[condition1]\n",
    "            for p in max_name:\n",
    "                new_df[p][i] = sa[p].max()\n",
    "        else :\n",
    "            condition2 = date_range[i-1]<x['time']\n",
    "            condition3 = date_range[i]>=x['time']\n",
    "            sa = x[(condition2)&(condition3)]\n",
    "\n",
    "            for p in max_name:\n",
    "                new_df[p][i] = sa[p].max()\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "result_1hr = result3.groupby('stay_id').apply(resampling_func_1hr)\n",
    "result_1hr = result_1hr.reset_index(drop=True)\n",
    "print('---------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1hr.to_csv('./1hr Shock Dataset resampled labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elevation_index(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    x['Troponin Elevation index'] = None\n",
    "    x['Creatinine Elevation index'] = None\n",
    "    x['Lactate Elevation index'] = None\n",
    "\n",
    "    #troponin\n",
    "    actual_trop_test_index = x[x['Actual Troponin test']==1].index\n",
    "    atti_ls = actual_trop_test_index.tolist()\n",
    "    #index를 list로 변환해서 길이만큼\n",
    "    if len(atti_ls)!=0:\n",
    "        x['Troponin Elevation index'][atti_ls[0]:] = 'maintain'\n",
    "    for i in range(len(atti_ls)-1):\n",
    "        before = x['Troponin'][atti_ls[i]]\n",
    "        after = x['Troponin'][atti_ls[i+1]]\n",
    "        if after > before*1.2:\n",
    "            x['Troponin Elevation index'][atti_ls[i+1]:] = 'up'\n",
    "        elif after < before*0.8:\n",
    "            x['Troponin Elevation index'][atti_ls[i+1]:] = 'down'\n",
    "        elif (after >= before*0.8) & (after <= before*1.2) :\n",
    "            x['Troponin Elevation index'][atti_ls[i+1]:] = 'maintain'          \n",
    "    x['Troponin Elevation index'].fillna('Unk', inplace=True)\n",
    "\n",
    "    #creatinine\n",
    "    actual_crea_test_index = x[x['Actual Creatinine test']==1].index\n",
    "    acti_ls = actual_crea_test_index.tolist()\n",
    "    #index를 list로 변환해서 길이만큼\n",
    "    if len(acti_ls)!=0:\n",
    "        x['Creatinine Elevation index'][acti_ls[0]:] = 'maintain'\n",
    "    for i in range(len(acti_ls)-1): \n",
    "        before = x['Creatinine'][acti_ls[i]]\n",
    "        after = x['Creatinine'][acti_ls[i+1]]\n",
    "        if after > before*1.1:\n",
    "            x['Creatinine Elevation index'][acti_ls[i+1]:] = 'up'\n",
    "        elif after < before*0.9:\n",
    "            x['Creatinine Elevation index'][acti_ls[i+1]:] = 'down'\n",
    "        elif (after >= before*0.9) & (after <= before*1.1) :\n",
    "            x['Creatinine Elevation index'][acti_ls[i+1]:] = 'maintain'\n",
    "    x['Creatinine Elevation index'].fillna('Unk', inplace=True)\n",
    "\n",
    "    #lactate\n",
    "    actual_Lactate_test_index = x[x['Actual Lactate test']==1].index\n",
    "    alti_ls = actual_Lactate_test_index.tolist()\n",
    "    #index를 list로 변환해서 길이만큼\n",
    "    if len(alti_ls)!=0:\n",
    "        x['Lactate Elevation index'][alti_ls[0]:] = 'maintain'\n",
    "    for i in range(len(alti_ls)-1):\n",
    "        before = x['Lactate'][alti_ls[i]]\n",
    "        after = x['Lactate'][alti_ls[i+1]]\n",
    "        if after > before*1.2:\n",
    "            x['Lactate Elevation index'][alti_ls[i+1]:] = 'up'\n",
    "        elif after < before*0.8:\n",
    "            x['Lactate Elevation index'][alti_ls[i+1]:] = 'down'\n",
    "        elif (after >= before*0.8) & (after <= before*1.2) :\n",
    "            x['Lactate Elevation index'][alti_ls[i+1]:] = 'maintain'\n",
    "    x['Lactate Elevation index'].fillna('Unk', inplace=True)\n",
    "        \n",
    "    return x\n",
    "\n",
    "result_1hr_elevation = result_1hr.groupby('stay_id').apply(add_elevation_index)\n",
    "result_1hr_elevation.reset_index(drop=True, inplace=True)\n",
    "print(result_1hr_elevation['stay_id'].unique().shape)\n",
    "print(result_1hr_elevation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1hr_elevation.to_csv('./1hr shock Dataset elevation labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_1hr_elevation = pd.read_csv('./1hr shock Dataset elevation labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1hr_elevation.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_readmission(x):\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    stay_id_num = len(x['stay_id'].unique())\n",
    "\n",
    "    if stay_id_num < 2:\n",
    "        x['Readmission'] = 0\n",
    "\n",
    "    else:\n",
    "        x['Readmission'] = 1\n",
    "        id_ls = x['stay_id'].unique().tolist()\n",
    "        new_df = pd.DataFrame(columns=['stay_id', 'time'])\n",
    "\n",
    "        for stay_id in id_ls:\n",
    "            first_adm_time = x.loc[x['stay_id'] == stay_id, 'time'].min()\n",
    "            new_df = new_df.append({'stay_id': stay_id, 'time': first_adm_time}, ignore_index=True)\n",
    "\n",
    "        new_df = new_df.sort_values(by=['time'], ascending=True)\n",
    "        first_adm_index = x[x['stay_id'] == new_df.iloc[0]['stay_id']].index\n",
    "        x.loc[first_adm_index, 'Readmission'] = 0\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "result_1hr_elevation = result_1hr_elevation.groupby('hadm_id').apply(find_readmission)\n",
    "result_1hr_elevation = result_1hr_elevation.sort_values(by=['stay_id','time'], ascending=[True, True])\n",
    "result_1hr_elevation.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_1hr_elevation['hadm_id'].unique().shape)\n",
    "print(result_1hr_elevation['stay_id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1hr_elevation[result_1hr_elevation['Readmission']==1].stay_id.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambiguous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ambiguous_data(x): \n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    column_list = ['Heart rate', 'Blood pressure systolic', 'Blood pressure diastolic', 'Respiratory rate', 'SpO2', 'Temperature', \n",
    "                    'Hemoglobin', 'Creatinine', 'Troponin', 'Lactate', 'Shock index']\n",
    "    \n",
    "    rows_with_nan = x[x[column_list].isna().any(axis=1)]\n",
    "\n",
    "    return rows_with_nan\n",
    "\n",
    "\n",
    "ambiguous_data = result_1hr_elevation.groupby('stay_id').apply(Ambiguous_data)\n",
    "print(ambiguous_data.shape)\n",
    "#imputated_data = 0\n",
    "ambiguous_data.reset_index(drop=True, inplace=True)\n",
    "print(ambiguous_data['stay_id'].unique().shape)\n",
    "\n",
    "ambiguous_data.to_csv('./Data preprocessed 4/1hr ambiguous Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Ambiguous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_Ambiguous_data(x): \n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "    column_list = ['Heart rate', 'Blood pressure systolic', 'Blood pressure diastolic', 'Respiratory rate', 'SpO2', 'Temperature', \n",
    "                    'Hemoglobin', 'Creatinine', 'Troponin', 'Lactate', 'Shock index']\n",
    "    \n",
    "    rows_without_nan = x[x[column_list].notna().all(axis=1)]\n",
    "\n",
    "    return rows_without_nan\n",
    "\n",
    "\n",
    "non_ambiguous_data = result_1hr_elevation.groupby('stay_id').apply(non_Ambiguous_data)\n",
    "print(non_ambiguous_data.shape)\n",
    "#imputated_data = 0\n",
    "non_ambiguous_data.reset_index(drop=True, inplace=True)\n",
    "print(non_ambiguous_data['stay_id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_data.to_csv('./1hr non_ambiguous_data Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "ambiguous_data = ambiguous_data.replace({'gender' : 'F'}, 0)\n",
    "ambiguous_data = ambiguous_data.replace({'gender' : 'M'}, 1)\n",
    "\n",
    "ambiguous_data = ambiguous_data.replace({'Troponin Elevation index' : 'maintain'}, 0)\n",
    "ambiguous_data = ambiguous_data.replace({'Troponin Elevation index' : 'up'}, 2)\n",
    "ambiguous_data = ambiguous_data.replace({'Troponin Elevation index' : 'down'}, -2)\n",
    "ambiguous_data = ambiguous_data.replace({'Troponin Elevation index' : 'Unk'}, 100)\n",
    "\n",
    "ambiguous_data = ambiguous_data.replace({'Creatinine Elevation index' : 'maintain'}, 0)\n",
    "ambiguous_data = ambiguous_data.replace({'Creatinine Elevation index' : 'up'}, 2)\n",
    "ambiguous_data = ambiguous_data.replace({'Creatinine Elevation index' : 'down'}, -2)\n",
    "ambiguous_data = ambiguous_data.replace({'Creatinine Elevation index' : 'Unk'}, 100)\n",
    "\n",
    "ambiguous_data = ambiguous_data.replace({'Lactate Elevation index' : 'maintain'}, 0)\n",
    "ambiguous_data = ambiguous_data.replace({'Lactate Elevation index' : 'up'}, 2)\n",
    "ambiguous_data = ambiguous_data.replace({'Lactate Elevation index' : 'down'}, -2)\n",
    "ambiguous_data = ambiguous_data.replace({'Lactate Elevation index' : 'Unk'}, 100)\n",
    "\n",
    "ambiguous_data['Elevation index type'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_data['Data type'] = 'Ambiguous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_data.to_csv('./1hr Shock Research ambiguous Dataset_type27.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "non_ambiguous_data = pd.read_csv('./Data preprocessed 4/1hr non_ambiguous_data Dataset.csv')\n",
    "\n",
    "data2 = non_ambiguous_data[['Troponin Elevation index', 'Creatinine Elevation index', 'Lactate Elevation index']]\n",
    "\n",
    "# 가능한 조합 생성\n",
    "combinations = list(itertools.product(['up', 'down', 'maintain'], repeat=len(data2.columns)))\n",
    "\n",
    "# 조합에 따라 값을 할당\n",
    "data2['Elevation index type'] = [combinations.index(tuple(row)) for row in data2.itertuples(index=False, name=None)]\n",
    "\n",
    "result_df = pd.DataFrame(data2)\n",
    "# 결과 확인\n",
    "combination_df = pd.DataFrame(combinations, columns=['Troponin Elevation index','Creatinine Elevation index','Lactate Elevation index'])\n",
    "combination_df.reset_index(drop=False, inplace=True)\n",
    "combination_df.rename(columns={'index':'Elevation index type'}, inplace=True)\n",
    "combination_df.to_csv('./Elevation index type_27.csv', index=False)\n",
    "\n",
    "print(result_df['Elevation index type'].unique().shape)\n",
    "\n",
    "elevation_index_ratio = result_df['Elevation index type'].value_counts()/len(result_df)\n",
    "\n",
    "elevation_index_ratio_df = pd.DataFrame(elevation_index_ratio)\n",
    "elevation_index_ratio_df.reset_index(drop=False, inplace=True)\n",
    "elevation_index_ratio_df.rename(columns={'Elevation index type':'Ratio'}, inplace=True)\n",
    "elevation_index_ratio_df.rename(columns={'index':'Elevation index type'}, inplace=True)\n",
    "\n",
    "under1 = elevation_index_ratio_df[elevation_index_ratio_df['Ratio']<0.01]\n",
    "over1_under10 = elevation_index_ratio_df[(elevation_index_ratio_df['Ratio']>=0.01)&(elevation_index_ratio_df['Ratio']<0.10)]\n",
    "over10_under20 = elevation_index_ratio_df[(elevation_index_ratio_df['Ratio']>=0.10)&(elevation_index_ratio_df['Ratio']<0.20)]\n",
    "over20 = elevation_index_ratio_df[elevation_index_ratio_df['Ratio']>=0.20]\n",
    "\n",
    "print(under1['Elevation index type'].unique().shape)\n",
    "print(over1_under10['Elevation index type'].unique().shape)\n",
    "print(over10_under20['Elevation index type'].unique().shape)\n",
    "print(over20['Elevation index type'].unique().shape)\n",
    "\n",
    "print(under1['Ratio'].sum())\n",
    "print(over1_under10['Ratio'].sum())\n",
    "print(over10_under20['Ratio'].sum())\n",
    "print(over20['Ratio'].sum())\n",
    "\n",
    "non_ambiguous_data['Elevation index type'] = result_df['Elevation index type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_data = non_ambiguous_data.replace({'gender' : 'F'}, 0)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'gender' : 'M'}, 1)\n",
    "\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Troponin Elevation index' : 'maintain'}, 0)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Troponin Elevation index' : 'up'}, 2)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Troponin Elevation index' : 'down'}, -2)\n",
    "\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Creatinine Elevation index' : 'maintain'}, 0)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Creatinine Elevation index' : 'up'}, 2)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Creatinine Elevation index' : 'down'}, -2)\n",
    "\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Lactate Elevation index' : 'maintain'}, 0)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Lactate Elevation index' : 'up'}, 2)\n",
    "non_ambiguous_data = non_ambiguous_data.replace({'Lactate Elevation index' : 'down'}, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_data['Data type'] = 'Non Ambiguous'\n",
    "non_ambiguous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ambiguous_data.to_csv('./1hr Shock Research non_ambiguous Dataset_type27.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#1hr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "x = ['Bin1','Bin2','Bin3','Bin4']\n",
    "y = [0.0407, 0.5835, 0, 0.3757]\n",
    "z = [10, 16, 0, 1]\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.barplot(x=y,y=x, width=0.5)\n",
    "plt.savefig(\"1hr Elevation index combination distribution_type27\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat ambiguous and non ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_data = pd.read_csv('./1hr Shock Research ambiguous Dataset_type27.csv')\n",
    "non_ambiguous_data = pd.read_csv('./1hr Shock Research non_ambiguous Dataset_type27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([ambiguous_data, non_ambiguous_data], axis=0, ignore_index=True)\n",
    "output = output.sort_values(by=['stay_id', 'time'], ascending=[True,True])\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "output.to_csv('./1hr Shock Research type27 for Cont_delta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
